{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ed52a4-3b8b-4fab-b1f0-fedde07837a9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c21a9-dc2c-43c9-9d0c-64aa73381d18",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "https://arxiv.org/pdf/2305.00586\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/georgiasimpression/.local/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import operator\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "from pyfunctions.general import compare_same\n",
    "from pyfunctions.cdt_basic import *\n",
    "from pyfunctions.cdt_source_to_target import *\n",
    "from pyfunctions.cdt_from_source_nodes import *\n",
    "from pyfunctions.toy_model import *\n",
    "from greater_than_task.greater_than_dataset import *\n",
    "from greater_than_task.utils import get_valid_years\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "Result = collections.namedtuple('Result', ('ablation_set', 'score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Load model and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a520f760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/georgiasimpression/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc77aff-a0ce-47bc-aa69-f9ced8df1497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/hannamw/gpt2-greater-than/blob/main/circuit_discovery.py; also these files came with their repo\n",
    "years_to_sample_from = get_valid_years(model.tokenizer, 1000, 1900)\n",
    "N = 490\n",
    "ds = YearDataset(years_to_sample_from, N, Path(\"../greater_than_task/cache/potential_nouns.txt\"), model.tokenizer, balanced=True, device=device, eos=True)\n",
    "year_indices = torch.load(\"../greater_than_task/cache/logit_indices.pt\")# .to(device)\n",
    "\n",
    "num_layers = len(model.blocks)\n",
    "seq_len = ds.good_toks.size()[-1]\n",
    "num_attention_heads = model.cfg.n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbf183-a5be-4d0b-83fd-75714a2241e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c93ab2-081a-4247-89c6-ab5a6ee434af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45b7d59f-e8c5-412f-b4af-9030d271fc3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e84af14c-d078-41b6-bd86-0b220b182217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|> The clash lasted from the year 1594 to the year 15', '<|endoftext|> The program lasted from the year 1395 to the year 13', '<|endoftext|> The challenge lasted from the year 1496 to the year 14', '<|endoftext|> The confrontation lasted from the year 1597 to the year 15', '<|endoftext|> The marriage lasted from the year 1098 to the year 10', '<|endoftext|> The journey lasted from the year 1202 to the year 12', '<|endoftext|> The insurgency lasted from the year 1803 to the year 18', '<|endoftext|> The improvement lasted from the year 1404 to the year 14', '<|endoftext|> The consultation lasted from the year 1705 to the year 17', '<|endoftext|> The domination lasted from the year 1606 to the year 16']\n",
      "tensor([ 405,  486, 2999, 3070, 3023, 2713, 3312, 2998, 2919, 2931,  940, 1157,\n",
      "        1065, 1485, 1415, 1314, 1433, 1558, 1507, 1129, 1238, 2481, 1828, 1954,\n",
      "        1731, 1495, 2075, 1983, 2078, 1959, 1270, 3132, 2624, 2091, 2682, 2327,\n",
      "        2623, 2718, 2548, 2670, 1821, 3901, 3682, 3559, 2598, 2231, 3510, 2857,\n",
      "        2780, 2920, 1120, 4349, 4309, 4310, 4051, 2816, 3980, 3553, 3365, 3270,\n",
      "        1899, 5333, 5237, 5066, 2414, 2996, 2791, 3134, 3104, 3388, 2154, 4869,\n",
      "        4761, 4790, 4524, 2425, 4304, 3324, 3695, 3720, 1795, 6659, 6469, 5999,\n",
      "        5705, 5332, 4521, 5774, 3459, 4531, 3829, 6420, 5892, 6052, 5824, 3865,\n",
      "        4846, 5607, 4089, 2079])\n",
      "['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "# print(ds)\n",
    "'''\n",
    "These guys weirdly implemented all the functionality of their class in class-level attributes\n",
    "years_to_sample_from: torch.Tensor\n",
    "    N: int\n",
    "    ordered: bool\n",
    "    eos: bool\n",
    "\n",
    "    nouns: List[str]\n",
    "    years: torch.Tensor\n",
    "    years_YY: torch.Tensor\n",
    "    good_sentences: List[str]\n",
    "    bad_sentences: List[str]\n",
    "    good_toks: torch.Tensor\n",
    "    bad_toks: torch.Tensor\n",
    "    good_prompt: List[str]\n",
    "    bad_prompt: List[str]\n",
    "    good_mask: torch.Tensor\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    '''\n",
    "\n",
    "# ds.N\n",
    "# ds.nouns\n",
    "# print(ds.years[:20]) # not sorted by XX for some reason\n",
    "# print(ds.years_YY[:]) # but does correspond to these YYs, which are mostly sorted\n",
    "print(ds.good_sentences[-10:]) # includes The endeavor lasted from the year 1098 to the year 10', but 1099 isn't in the list of years?\n",
    "# note: we want prediction at the last token, unlike with the IOI dataset where we want second-to-last\n",
    "# i checked and there is no internal logic to prevent such sentences from being produced, so i guess we're SOL if we sample one?\n",
    "# print(ds.bad_sentences[-10:]) # these all start with 01, e.g 1601 to. they're bad because there is no possible incorrect input\n",
    "# print(ds.good_mask.size()) # n, 100 (100 different years)\n",
    "# print(ds.good_toks.size()) # n, 13\n",
    "# print(ds.bad_toks.size()) # there isn't any necessary correspondence, N is just the number of good sequences and bad sequences alike\n",
    "# list(ds.years.cpu().numpy()).index(1099)\n",
    "print(year_indices)\n",
    "print(model.tokenizer.convert_ids_to_tokens(year_indices)) # length 100, starts with index for '00' and ends with index for '99', great\n",
    "# print(model.tokenizer.decode(year_indices, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f3a6e99-a858-431b-a96b-815c9622f72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.good_mask[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b2f8b4-0513-4663-8cd3-1dccffca20f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> The attempts lasted from the year 1603 to the year 16'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.good_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a241453d-5471-4547-9bc3-d370c570e212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.good_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46ac8f-a1b0-40c5-bd07-ceb27d474497",
   "metadata": {},
   "source": [
    "The task specific objective here is something like (sum of probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702b0cf-f9e6-4386-9603-1b35924cb129",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Setup attention mask and mean activations for ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f54e51e-77a7-4732-885d-d58f9eba9842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/georgiasimpression/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:1080: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.tensor([1 for x in range(seq_len)]).view(1, -1).to(device)\n",
    "input_shape = ds.good_toks[0:1, :].size() # by making the sample size 1, you can get an extended attention mask with batch size 1, which will broadcast\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a83177e-9605-4f53-8067-d4a3ae4f22f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del logits\n",
    "del cache\n",
    "import gc\n",
    "gc.collect()\n",
    "model.cfg.use_attn_result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68a4e59-e9ec-4521-b66b-6ff14330bdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([490, 12, 13, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 13, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, cache = model.run_with_cache(ds.good_toks) # run on entire dataset along batch dimension\n",
    "\n",
    "attention_outputs = [cache['blocks.' + str(i) + '.attn.hook_z'] for i in range(num_attention_heads)]\n",
    "attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, layer, seq, n_heads, dim_attn\n",
    "print(attention_outputs.shape)\n",
    "mean_acts = torch.mean(attention_outputs, dim=0)\n",
    "old_shape = mean_acts.shape\n",
    "last_dim = old_shape[-2] * old_shape[-1]\n",
    "new_shape = old_shape[:-2] + (last_dim,)\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "mean_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6aa206-1608-4a4e-aa73-24ffcbadb3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick check for equality, particularly to make sure we've made the attention mask correctly\n",
    "ranges = [\n",
    "        [layer for layer in range(num_layers)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        [attention_head_idx for attention_head_idx in range(num_attention_heads)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "target_nodes = []\n",
    "out_decomp, _, _, _ = prop_GPT(ds.good_toks[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=None, set_irrel_to_mean=False)\n",
    "\n",
    "logits, cache = model.run_with_cache(ds.good_toks[0])\n",
    "\n",
    "compare_same(out_decomp[0].rel + out_decomp[0].irrel, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1eda4-a2c2-4b35-b95e-ab1cc2149be9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Loose experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "159db1cb-e612-4dda-87cb-6e4bbbff075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 20\n",
    "sample_idxs = random.sample(range(N), NUM_SAMPLES)\n",
    "# sample_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e9dd30c-39a8-48b8-9cb1-a712ca65605b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> The pursuit lasted from the year 1290 to the year 12\n"
     ]
    }
   ],
   "source": [
    "print (ds.good_sentences[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58a48d12-ed55-491e-b61d-245e66635bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', '<|endoftext|>', ' The', ' pursuit', ' lasted', ' from', ' the', ' year', ' 12', '90', ' to', ' the', ' year', ' 12']\n",
      "Tokenized answer: ['03']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.55</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.22</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m52\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m20.55\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.22\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m03\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 25.19 Prob: 22.77% Token: |90|\n",
      "Top 1th token. Logit: 24.12 Prob:  7.78% Token: |99|\n",
      "Top 2th token. Logit: 23.77 Prob:  5.51% Token: |94|\n",
      "Top 3th token. Logit: 23.73 Prob:  5.31% Token: |95|\n",
      "Top 4th token. Logit: 23.63 Prob:  4.81% Token: |92|\n",
      "Top 5th token. Logit: 23.30 Prob:  3.44% Token: |60|\n",
      "Top 6th token. Logit: 23.22 Prob:  3.18% Token: |98|\n",
      "Top 7th token. Logit: 23.18 Prob:  3.06% Token: |96|\n",
      "Top 8th token. Logit: 23.14 Prob:  2.94% Token: |50|\n",
      "Top 9th token. Logit: 23.10 Prob:  2.82% Token: |91|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'03'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'03'\u001b[0m, \u001b[1;36m52\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = ds.good_sentences[88] # GPT2 doesn't always perform this task correctly, only about 98% of the time.\n",
    "# On example input <|endoftext|> The pursuit lasted from the year 1290 to the year 12 , the top prediction is '90'.\n",
    "example_answer = '03'\n",
    "\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b4e0769-739d-460d-bb4e-1ced376a28d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is not a pure function, it's a closure. It depends on ds.good_mask, sample_idxs, and year_indices.\n",
    "def score_logits(logits, sample_idxs_0):\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits[:, -1, :], device='cpu'), dim=-1).numpy() # sad\n",
    "    probs_for_year_tokens = probs[:, year_indices.cpu().numpy()]\n",
    "    probs_for_correct_years = probs_for_year_tokens[ds.good_mask.cpu().numpy()[sample_idxs_0]]\n",
    "    correct_score = np.sum(probs_for_correct_years)\n",
    "    probs_for_incorrect_years = probs_for_year_tokens[np.logical_not(ds.good_mask.cpu().numpy()[sample_idxs_0])]\n",
    "    incorrect_score = np.sum(probs_for_incorrect_years)\n",
    "    return (correct_score - incorrect_score) / len(sample_idxs_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80318fda-1f1c-4c85-a2c4-d338ecac0bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running input 0\n",
      "running input 600\n",
      "running input 1200\n",
      "running input 1800\n"
     ]
    }
   ],
   "source": [
    "mean_acts = mean_acts.view(new_shape)\n",
    "ranges = [\n",
    "        [layer for layer in range(num_layers)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        [attention_head_idx for attention_head_idx in range(num_attention_heads)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "target_nodes = []\n",
    "\n",
    "# cache activations for faster batch run\n",
    "out_decomp, _, _, pre_layer_activations = prop_GPT(ds.good_toks[sample_idxs, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(ds.good_toks[sample_idxs, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets, num_at_time=64 // len(sample_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3819c7ec-a856-4236-ad3e-dfb0fd749883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_logits_decomposition_scores(out_decomps, normalized=False):\n",
    "    logits = (out_decomps[0].rel + out_decomps[0].irrel) # 1, seq_len, 50257=d_vocab\n",
    "    full_score = score_logits(logits)\n",
    "    assert(full_score > 0) # this needs to be replaced with a check higher in the pipeline; GPT2 succeeds at this like 99%+ of the time but not always\n",
    "    \n",
    "    results = []\n",
    "    relevances = np.zeros((num_layers, seq_len, num_attention_heads))\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        for seq_pos in range(seq_len):\n",
    "            for head_idx in range(num_attention_heads):\n",
    "                decomp = out_decomps[layer_idx * seq_len * num_attention_heads + seq_pos * num_attention_heads + head_idx]\n",
    "                score = score_logits(decomp.rel)\n",
    "                norm_score = score / full_score\n",
    "                relevances[layer_idx, seq_pos, head_idx] = norm_score\n",
    "                if not normalized:\n",
    "                    results.append(Result(decomp.ablation_set, norm_score))\n",
    "    if normalized:\n",
    "        sums_per_layer = np.sum(relevances, axis=(1, 2))\n",
    "        print(sums_per_layer)\n",
    "\n",
    "        sums_per_layer[sums_per_layer == 0] = -1e-8\n",
    "        relevances = relevances / np.expand_dims(sums_per_layer, (1, 2))\n",
    "        for layer_idx in range(num_layers):\n",
    "            for seq_pos in range(seq_len):\n",
    "                for head_idx in range(num_attention_heads):\n",
    "                    target_decomp = target_decomps[layer_idx * seq_len * num_attention_heads + seq_pos * num_attention_heads + head_idx]\n",
    "                    results.append(Result(target_decomp.ablation_set, relevances[layer_idx, seq_pos, head_idx]))\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "\n",
    "    return results, relevances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c397ecd2-9d0d-4edc-b795-de7b94484fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_decomps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# results = compute_logits_decomposition_scores(out_decomps)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results, relevances \u001b[38;5;241m=\u001b[39m compute_logits_decomposition_scores(\u001b[43mout_decomps\u001b[49m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results[:\u001b[38;5;241m20\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_decomps' is not defined"
     ]
    }
   ],
   "source": [
    "# results = compute_logits_decomposition_scores(out_decomps)\n",
    "results, relevances = compute_logits_decomposition_scores(out_decomps, normalized=True)\n",
    "for result in results[:20]:\n",
    "    print(result)\n",
    "    # print(result.ablation_set[0])\n",
    "'''\n",
    " a9.h1, while\n",
    "MLP 8 relies on a8.h11, a8.h8, a7.h10, a6.h9, a5.h5, and a5.h1\n",
    "\n",
    "(9, 1), (8, 11), (8, 8), (7, 10), (6, 9), (5, 5), (5, 1)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "32b2b05c-eee7-4a95-aef5-126b082ec550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'sequence_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m source_nodes \u001b[38;5;241m=\u001b[39m [Node(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mranges)]\n\u001b[1;32m     10\u001b[0m ablation_sets \u001b[38;5;241m=\u001b[39m [(n,) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m source_nodes]\n\u001b[0;32m---> 12\u001b[0m _, _, _, pre_layer_activations \u001b[38;5;241m=\u001b[39m \u001b[43mprop_GPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgood_toks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mablation_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_irrel_to_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m prop_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m ablation_list: prop_GPT(ds\u001b[38;5;241m.\u001b[39mgood_toks[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :], extended_attention_mask, model, ablation_list, target_nodes\u001b[38;5;241m=\u001b[39mtarget_nodes, device\u001b[38;5;241m=\u001b[39mdevice, mean_acts\u001b[38;5;241m=\u001b[39mmean_acts, set_irrel_to_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cached_pre_layer_acts\u001b[38;5;241m=\u001b[39mpre_layer_activations)\n\u001b[1;32m     15\u001b[0m out_decomps, target_decomps \u001b[38;5;241m=\u001b[39m batch_run(prop_fn, ablation_sets)\n",
      "File \u001b[0;32m~/CD_Circuit/pyfunctions/cdt_source_to_target.py:455\u001b[0m, in \u001b[0;36mprop_GPT\u001b[0;34m(encoding_idxs, extended_attention_mask, model, ablation_list, target_nodes, device, mean_acts, att_list, set_irrel_to_mean, cached_pre_layer_acts, target_decomp_method)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     layer_mean_acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m rel, irrel, layer_target_decomps, returned_att_probs \u001b[38;5;241m=\u001b[39m \u001b[43mprop_GPT_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mirrel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mablation_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mlayer_mean_acts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mlayer_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43matt_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mset_irrel_to_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_irrel_to_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_decomp_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_decomp_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(target_decomps)):\n\u001b[1;32m    464\u001b[0m     target_decomps[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m layer_target_decomps[idx]\n",
      "File \u001b[0;32m~/CD_Circuit/pyfunctions/cdt_source_to_target.py:253\u001b[0m, in \u001b[0;36mprop_GPT_layer\u001b[0;34m(rel, irrel, attention_mask, head_mask, ablation_dict, target_nodes, level, layer_mean_acts, layer_module, device, att_probs, set_irrel_to_mean, target_decomp_method)\u001b[0m\n\u001b[1;32m    250\u001b[0m rel_summed_values, irrel_summed_values \u001b[38;5;241m=\u001b[39m set_rel_at_source_nodes(rel_summed_values, irrel_summed_values, ablation_dict, layer_mean_acts, level, attn_wrapper, set_irrel_to_mean, device)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_decomp_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     layer_target_decomps \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_contributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_summed_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mirrel_summed_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mablation_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mattn_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m rel_attn_residual, irrel_attn_residual \u001b[38;5;241m=\u001b[39m prop_linear(rel_summed_values, irrel_summed_values, attn_wrapper\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m    260\u001b[0m rel_mid, irrel_mid \u001b[38;5;241m=\u001b[39m rel \u001b[38;5;241m+\u001b[39m rel_attn_residual, irrel \u001b[38;5;241m+\u001b[39m irrel_attn_residual\n",
      "File \u001b[0;32m~/CD_Circuit/pyfunctions/cdt_source_to_target.py:21\u001b[0m, in \u001b[0;36mcalculate_contributions\u001b[0;34m(rel, irrel, ablation_dict, target_nodes, level, sa_module, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     target_decomps_for_ablation \u001b[38;5;241m=\u001b[39m TargetNodeDecompositionList(ablation)            \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target_nodes_at_level:\n\u001b[0;32m---> 21\u001b[0m         target_decomps_for_ablation\u001b[38;5;241m.\u001b[39mappend(t, rel[batch_indices, \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_idx\u001b[49m, t\u001b[38;5;241m.\u001b[39mattn_head_idx, :],\n\u001b[1;32m     22\u001b[0m                                             irrel[batch_indices, t\u001b[38;5;241m.\u001b[39msequence_idx, t\u001b[38;5;241m.\u001b[39mattn_head_idx, :])\n\u001b[1;32m     23\u001b[0m     target_decomps\u001b[38;5;241m.\u001b[39mappend(target_decomps_for_ablation)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_decomps\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'sequence_idx'"
     ]
    }
   ],
   "source": [
    "\n",
    "target_nodes = [Node(9, 12, 1), Node(10, 12, 4)] # (10, 12, 7), (7, 12, 10)\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(16)],\n",
    "        # [ioi_dataset.word_idx['IO'][0]],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "\n",
    "_, _, _, pre_layer_activations = prop_GPT(ds.good_toks[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(ds.good_toks[0:1, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ebab7b5-5f80-40b6-98f1-bccf4a16274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_target_decomposition_scores(target_decomps, method=\"l1\", mean_acts=None, attn_cache=None):\n",
    "    results = []\n",
    "    # for target_decomp in target_decomps:\n",
    "\n",
    "    # this is just the same as what we did on BERT, seems like we take the ratio of the l1 norm of rel to l1 norm of irrel, summed over target nodes\n",
    "    # other ideas to try:\n",
    "    # normalize mean relevance by layer # does get rid of the \"early layers thing\", but many nodes are about as relevant as the ones identified by IOI\n",
    "    # try a better target decomposition metric: for example, dot product with the rel of the previous iter (which makes more sense)? # actually made the results worszze?\n",
    "    # try some measure of indirect conections rather than just direct connections\n",
    "    relevances = np.zeros((num_layers, seq_len, num_attention_heads))\n",
    "    for layer in range(num_layers):\n",
    "        for sequence_position in range(seq_len):\n",
    "            for attention_head_idx in range(num_attention_heads):\n",
    "                idx = layer * 16 * 12 + sequence_position * 12 + attention_head_idx\n",
    "                target_decomp = target_decomps[idx]\n",
    "                if target_decomp.ablation_set[0] in target_nodes:\n",
    "                    continue\n",
    "                score = 0\n",
    "                for i in range(len(target_decomp.target_nodes)):\n",
    "                    if method == 'l1':\n",
    "                        rels_magnitude = torch.mean(abs(target_decomp.rels[i])) # np.mean if you are on cpu\n",
    "                        irrels_magnitude = torch.mean(abs(target_decomp.irrels[i])) # np.mean if you are on cpu\n",
    "                        target_node_score = rels_magnitude / (rels_magnitude + irrels_magnitude)\n",
    "                        score += target_node_score\n",
    "                    if method == 'dot':\n",
    "                        target_node = target_decomp.target_nodes[i]\n",
    "                        # this method is only implemented for a single datapoint\n",
    "                        if mean_acts is None or attn_cache is None:\n",
    "                            print(\"Invalid target decomposition score calculation\") # and then this is going to crash anyway\n",
    "                        target_mean_act = mean_acts[target_node.layer_idx, target_node.sequence_idx, target_node.attn_head_idx]\n",
    "                        target_rel = attn_cache['blocks.' + str(target_node.layer_idx) + '.attn.hook_z'][0][target_node.sequence_idx][target_node.attn_head_idx] - target_mean_act \n",
    "                        rel = target_decomp.rels[i][0]\n",
    "                        #print(target_rel.shape, rel.shape)\n",
    "                        score += torch.dot(rel, target_rel)\n",
    "                relevances[layer, sequence_position, attention_head_idx] = score\n",
    "\n",
    "\n",
    "    sums_per_layer = np.sum(relevances, axis=(1, 2))\n",
    "    sums_per_layer[sums_per_layer == 0] = -1e-8\n",
    "    normalized_relevances = relevances / np.expand_dims(sums_per_layer, (1, 2))\n",
    "\n",
    "    num_layers = 12\n",
    "    seq_len = 16\n",
    "    num_attention_heads = 12\n",
    "    for layer_idx in range(num_layers):\n",
    "        for seq_pos in range(seq_len):\n",
    "            for head_idx in range(num_attention_heads):\n",
    "                target_decomp = target_decomps[layer_idx * seq_len * num_attention_heads + seq_pos * num_attention_heads + head_idx]\n",
    "                results.append(Result(target_decomp.ablation_set, normalized_relevances[layer_idx, seq_pos, head_idx]))\n",
    "\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5db37-3c6c-407a-a7b8-86e2723c2f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15bb0c2f-bef4-4bc3-8f5a-f2177a17e538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(layer_idx=9, sequence_idx=14, attn_head_idx=9)\n",
      "Node(layer_idx=10, sequence_idx=14, attn_head_idx=10)\n",
      "Node(layer_idx=9, sequence_idx=14, attn_head_idx=6)\n",
      "Node(layer_idx=9, sequence_idx=2, attn_head_idx=6)\n",
      "Node(layer_idx=0, sequence_idx=1, attn_head_idx=1)\n",
      "Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n"
     ]
    }
   ],
   "source": [
    "all_nodes = []\n",
    "for it in outliers_per_iter:\n",
    "    for result in it:\n",
    "        if result.ablation_set[0] not in all_nodes:\n",
    "            all_nodes.append(result.ablation_set[0])\n",
    "for node in all_nodes:\n",
    "    print((node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a438ad2b-5fa8-4211-bed6-c0d70f152341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then, Vanessa and Paul went to the house. Vanessa gave a basketball to Paul\n",
      "Then, Jessica and Lindsay went to the school. Jessica gave a snack to Lindsay\n"
     ]
    }
   ],
   "source": [
    "print(ioi_dataset.sentences[0])\n",
    "print(test_ioi_dataset.sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cb13f-4b1f-4bd0-9b8b-1a3bccbfe9ad",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Circuit evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b2ad668-f0bc-492b-9afd-92c5716b2d95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9669.96875\n",
      "10810.0\n",
      "1238.126953125\n",
      "7500.0\n"
     ]
    }
   ],
   "source": [
    "# del out_decomps\n",
    "# del target_decomps\n",
    "print(torch.cuda.memory_allocated(0)/1024/1024)\n",
    "print(torch.cuda.memory_reserved(0)/1024/1024)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(0)/1024/1024)\n",
    "print(torch.cuda.memory_reserved(0)/1024/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33374a25-c2a8-493e-8480-ecc2274e6ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranges = [\n",
    "        [layer for layer in range(num_layers)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        # [ioi_dataset.word_idx['IO'][0]],\n",
    "        [attention_head_idx for attention_head_idx in range(num_attention_heads)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "random_circuit = random.sample(source_nodes, 20)\n",
    "\n",
    "# sample_idxs = random.sample(range(N), NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5fe00b8-8a79-49f4-bb0d-26af18a46525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implicitly depends on year_indices/\n",
    "def correctness_rate(logits, sample_idxs_0):\n",
    "    logits_for_year_tokens = logits[:, -1, year_indices]\n",
    "    predicted_year_idxs = np.argmax(logits_for_year_tokens.cpu().numpy(), axis=-1)\n",
    "    # print(predicted_year_idxs.shape)\n",
    "    correct_per_input = ds.good_mask.cpu().numpy()[sample_idxs_0, predicted_year_idxs]\n",
    "    return np.sum(correct_per_input) / len(sample_idxs_0)\n",
    "    '''\n",
    "    probs_for_year_tokens = probs[:, year_indices.cpu().numpy()]\n",
    "    probs_for_correct_years = probs_for_year_tokens[ds.good_mask.cpu().numpy()[sample_idxs_0]]\n",
    "    correct_score = np.sum(probs_for_correct_years)\n",
    "    probs_for_incorrect_years = probs_for_year_tokens[np.logical_not(ds.good_mask.cpu().numpy()[sample_idxs_0])]\n",
    "    incorrect_score = np.sum(probs_for_incorrect_years)\n",
    "    return (correct_score - incorrect_score) / len(sample_idxs_0)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de2cde63-196a-46e8-8fa7-8f10cbf60be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsimply results from first iter\\ncircuit = [Node(layer_idx=9, sequence_idx=12, attn_head_idx=1),\\n    Node(layer_idx=10, sequence_idx=12, attn_head_idx=4),\\n    Node(layer_idx=10, sequence_idx=12, attn_head_idx=7),\\n    Node(layer_idx=7, sequence_idx=12, attn_head_idx=10),\\n]\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = []\n",
    "for (layer_idx, head_idx) in [(9, 1), (8, 11), (8, 8), (7, 10), (6, 9), (5, 5), (5, 1)]: # greater-than paper's result\n",
    "    for seq_pos in range(seq_len):\n",
    "        circuit.append(Node(layer_idx, seq_pos, head_idx))\n",
    "'''\n",
    "simply results from first iter\n",
    "circuit = [Node(layer_idx=9, sequence_idx=12, attn_head_idx=1),\n",
    "    Node(layer_idx=10, sequence_idx=12, attn_head_idx=4),\n",
    "    Node(layer_idx=10, sequence_idx=12, attn_head_idx=7),\n",
    "    Node(layer_idx=7, sequence_idx=12, attn_head_idx=10),\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9db57878-aaf0-4ef6-83ad-021902bf9e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2482656/1730026890.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probs = torch.nn.functional.softmax(torch.tensor(logits[:, -1, :], device='cpu'), dim=-1).numpy() # sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7653578349522183\n",
      "0.9857142857142857\n"
     ]
    }
   ],
   "source": [
    "from pyfunctions.faithfulness_ablations import add_mean_ablation_hook\n",
    "\n",
    "mean_acts = mean_acts.view(old_shape)\n",
    "# print(mean_acts.shape)\n",
    "model.reset_hooks(including_permanent=True)\n",
    "# current findings:\n",
    "# full model: 0.817, 0.989 correctness\n",
    "# ablate all attention layers entirely: 0.515, 0.891\n",
    "# random circuit of 20 \"head, seq_pos\": 0.532, 0.891\n",
    "# our \"four head, seq_pos\" circuit: 0.711, 0.955\n",
    "# their circuit: 0.765, 0.985\n",
    "model = add_mean_ablation_hook(model, patch_values=mean_acts, circuit=circuit)\n",
    "# batching\n",
    "NUM_AT_TIME = 64\n",
    "start_idx = 0\n",
    "score = 0\n",
    "correctness = 0\n",
    "while True:\n",
    "    end_idx = start_idx + NUM_AT_TIME\n",
    "    if end_idx > N:\n",
    "        end_idx = N\n",
    "    \n",
    "    logits, cache = model.run_with_cache(ds.good_toks[start_idx:end_idx]) # run on entire dataset along batch dimension\n",
    "    batch_score = score_logits(logits, range(start_idx, end_idx))\n",
    "    batch_correctness_rate = correctness_rate(logits, range(start_idx, end_idx))\n",
    "    num_samples = end_idx - start_idx\n",
    "    score += batch_score * (num_samples / N)\n",
    "    correctness += batch_correctness_rate * (num_samples / N)\n",
    "    start_idx += NUM_AT_TIME\n",
    "    if end_idx == N:\n",
    "        break\n",
    "print(score)\n",
    "print(correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea429b1a-516d-4210-a0c7-7278aae0f8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tentatively improved score to -1.255778   by removing node  Node(layer_idx=10, sequence_idx=14, attn_head_idx=10)\n",
      "removing  Node(layer_idx=10, sequence_idx=14, attn_head_idx=10)  to achieve score of -1.255778\n",
      "tentatively improved score to -1.220982   by removing node  Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n",
      "removing  Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)  to achieve score of -1.220982\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# speculative: try to generate a better circuit by greedy search\n",
    "\n",
    "NAME_MOVER_HEADS = [Node(9, 14, 9), Node(10, 14, 0), Node(9, 14, 6)]\n",
    "old_circuit = circuit.copy()\n",
    "best_score = -1.4686 # \n",
    "while True:\n",
    "    node_to_remove = None\n",
    "    for idx, node in enumerate(circuit):\n",
    "        if node in NAME_MOVER_HEADS:\n",
    "            continue\n",
    "        new_circuit = circuit.copy()\n",
    "        new_circuit.remove(node)\n",
    "        # print(new_circuit)\n",
    "        model.reset_hooks(including_permanent=True)\n",
    "        model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=new_circuit)\n",
    "        logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "        ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset).cpu().numpy().item()\n",
    "        if ave_logit_diff > best_score:\n",
    "            best_score = ave_logit_diff\n",
    "            node_to_remove = node\n",
    "            print('tentatively improved score to %f ' % best_score, ' by removing node ', node_to_remove)\n",
    "    if node_to_remove is None: \n",
    "        # then we can't improve any further so the algorithm terminates\n",
    "        break\n",
    "    print(\"removing \", node_to_remove, \" to achieve score of %f\" % best_score)\n",
    "    circuit.remove(node_to_remove)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e9717bb-1f4e-4823-bcfc-ea685daf8af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5994, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "# model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=circuit)\n",
    "model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=nodes)\n",
    "logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset)\n",
    "print(ave_logit_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
