{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ed52a4-3b8b-4fab-b1f0-fedde07837a9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c21a9-dc2c-43c9-9d0c-64aa73381d18",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "https://arxiv.org/pdf/2305.00586\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/georgiasimpression/.local/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import operator\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "from pyfunctions.general import compare_same\n",
    "from pyfunctions.cdt_basic import *\n",
    "from pyfunctions.cdt_source_to_target import *\n",
    "from pyfunctions.cdt_from_source_nodes import *\n",
    "from pyfunctions.cdt_ablations import *\n",
    "from pyfunctions.cdt_core import *\n",
    "from pyfunctions.toy_model import *\n",
    "from pyfunctions.faithfulness_ablations import add_mean_ablation_hook\n",
    "\n",
    "from greater_than_task.greater_than_dataset import *\n",
    "from greater_than_task.utils import get_valid_years\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "Result = collections.namedtuple('Result', ('ablation_set', 'score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Load model and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a520f760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/georgiasimpression/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc77aff-a0ce-47bc-aa69-f9ced8df1497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/hannamw/gpt2-greater-than/blob/main/circuit_discovery.py; also these files came with their repo\n",
    "years_to_sample_from = get_valid_years(model.tokenizer, 1000, 1900)\n",
    "N = 5000\n",
    "ds = YearDataset(years_to_sample_from, N, Path(\"../greater_than_task/cache/potential_nouns.txt\"), model.tokenizer, balanced=True, device=device, eos=True)\n",
    "year_indices = torch.load(\"../greater_than_task/cache/logit_indices.pt\")# .to(device)\n",
    "\n",
    "num_layers = len(model.blocks)\n",
    "seq_len = ds.good_toks.size()[-1]\n",
    "num_attention_heads = model.cfg.n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbf183-a5be-4d0b-83fd-75714a2241e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c93ab2-081a-4247-89c6-ab5a6ee434af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e84af14c-d078-41b6-bd86-0b220b182217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|> The clash lasted from the year 1594 to the year 15', '<|endoftext|> The program lasted from the year 1395 to the year 13', '<|endoftext|> The challenge lasted from the year 1496 to the year 14', '<|endoftext|> The confrontation lasted from the year 1597 to the year 15', '<|endoftext|> The marriage lasted from the year 1098 to the year 10', '<|endoftext|> The journey lasted from the year 1202 to the year 12', '<|endoftext|> The insurgency lasted from the year 1803 to the year 18', '<|endoftext|> The improvement lasted from the year 1404 to the year 14', '<|endoftext|> The consultation lasted from the year 1705 to the year 17', '<|endoftext|> The domination lasted from the year 1606 to the year 16']\n",
      "tensor([ 405,  486, 2999, 3070, 3023, 2713, 3312, 2998, 2919, 2931,  940, 1157,\n",
      "        1065, 1485, 1415, 1314, 1433, 1558, 1507, 1129, 1238, 2481, 1828, 1954,\n",
      "        1731, 1495, 2075, 1983, 2078, 1959, 1270, 3132, 2624, 2091, 2682, 2327,\n",
      "        2623, 2718, 2548, 2670, 1821, 3901, 3682, 3559, 2598, 2231, 3510, 2857,\n",
      "        2780, 2920, 1120, 4349, 4309, 4310, 4051, 2816, 3980, 3553, 3365, 3270,\n",
      "        1899, 5333, 5237, 5066, 2414, 2996, 2791, 3134, 3104, 3388, 2154, 4869,\n",
      "        4761, 4790, 4524, 2425, 4304, 3324, 3695, 3720, 1795, 6659, 6469, 5999,\n",
      "        5705, 5332, 4521, 5774, 3459, 4531, 3829, 6420, 5892, 6052, 5824, 3865,\n",
      "        4846, 5607, 4089, 2079])\n",
      "['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "# print(ds)\n",
    "'''\n",
    "These guys weirdly implemented all the functionality of their class in class-level attributes\n",
    "years_to_sample_from: torch.Tensor\n",
    "    N: int\n",
    "    ordered: bool\n",
    "    eos: bool\n",
    "\n",
    "    nouns: List[str]\n",
    "    years: torch.Tensor\n",
    "    years_YY: torch.Tensor\n",
    "    good_sentences: List[str]\n",
    "    bad_sentences: List[str]\n",
    "    good_toks: torch.Tensor\n",
    "    bad_toks: torch.Tensor\n",
    "    good_prompt: List[str]\n",
    "    bad_prompt: List[str]\n",
    "    good_mask: torch.Tensor\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    '''\n",
    "\n",
    "# ds.N\n",
    "# ds.nouns\n",
    "# print(ds.years[:20]) # not sorted by XX for some reason\n",
    "# print(ds.years_YY[:]) # but does correspond to these YYs, which are mostly sorted\n",
    "print(ds.good_sentences[-10:]) # includes The endeavor lasted from the year 1098 to the year 10', but 1099 isn't in the list of years?\n",
    "# note: we want prediction at the last token, unlike with the IOI dataset where we want second-to-last\n",
    "# i checked and there is no internal logic to prevent such sentences from being produced, so i guess we're SOL if we sample one?\n",
    "# print(ds.bad_sentences[-10:]) # these all start with 01, e.g 1601 to. they're bad because there is no possible incorrect input\n",
    "# print(ds.good_mask.size()) # n, 100 (100 different years)\n",
    "# print(ds.good_toks.size()) # n, 13\n",
    "# print(ds.bad_toks.size()) # there isn't any necessary correspondence, N is just the number of good sequences and bad sequences alike\n",
    "# list(ds.years.cpu().numpy()).index(1099)\n",
    "print(year_indices)\n",
    "print(model.tokenizer.convert_ids_to_tokens(year_indices)) # length 100, starts with index for '00' and ends with index for '99', great\n",
    "# print(model.tokenizer.decode(year_indices, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702b0cf-f9e6-4386-9603-1b35924cb129",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Setup attention mask and mean activations for ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f54e51e-77a7-4732-885d-d58f9eba9842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/binyu/georgiasimpression/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:1080: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.tensor([1 for x in range(seq_len)]).view(1, -1).to(device)\n",
    "input_shape = ds.good_toks[0:1, :].size() # by making the sample size 1, you can get an extended attention mask with batch size 1, which will broadcast\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68a4e59-e9ec-4521-b66b-6ff14330bdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 13, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_AT_TIME = 64\n",
    "start_idx = 0\n",
    "score = 0\n",
    "correctness = 0\n",
    "\n",
    "all_attention_outputs = []\n",
    "while True:\n",
    "    end_idx = start_idx + NUM_AT_TIME\n",
    "    if end_idx > N:\n",
    "        end_idx = N\n",
    "    logits, cache = model.run_with_cache(ds.good_toks[start_idx:end_idx]) # run on entire dataset along batch dimension\n",
    "    attention_outputs = [cache['blocks.' + str(i) + '.attn.hook_z'] for i in range(num_attention_heads)]\n",
    "    attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, layer, seq, n_heads, dim_attn\n",
    "    all_attention_outputs.append(attention_outputs)\n",
    "\n",
    "    start_idx += NUM_AT_TIME\n",
    "    if end_idx == N:\n",
    "        break\n",
    "all_attention_outputs = torch.cat(all_attention_outputs, dim=0)\n",
    "mean_acts = torch.mean(all_attention_outputs, dim=0)\n",
    "old_shape = mean_acts.shape\n",
    "last_dim = old_shape[-2] * old_shape[-1]\n",
    "new_shape = old_shape[:-2] + (last_dim,)\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "mean_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f6aa206-1608-4a4e-aa73-24ffcbadb3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of the values are equal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check for equality, particularly to make sure we've made the attention mask correctly\n",
    "ranges = [\n",
    "        [layer for layer in range(num_layers)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        [attention_head_idx for attention_head_idx in range(num_attention_heads)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "target_nodes = []\n",
    "out_decomp, _, _, _ = prop_GPT(ds.good_toks[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=None, set_irrel_to_mean=False)\n",
    "\n",
    "logits, cache = model.run_with_cache(ds.good_toks[0])\n",
    "\n",
    "compare_same(out_decomp[0].rel + out_decomp[0].irrel, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1eda4-a2c2-4b35-b95e-ab1cc2149be9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "159db1cb-e612-4dda-87cb-6e4bbbff075e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 100\n",
    "sample_idxs = random.sample(range(N), NUM_SAMPLES) # you actually have to sample randomly from this dataset because they are arranged in increasing order of YY token\n",
    "# sample_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a48d12-ed55-491e-b61d-245e66635bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', '<|endoftext|>', ' The', ' custody', ' lasted', ' from', ' the', ' year', ' 17', '90', ' to', ' the', ' year', ' 17']\n",
      "Tokenized answer: ['03']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.27</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m57\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m20.27\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m03\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 27.91 Prob: 19.42% Token: |90|\n",
      "Top 1th token. Logit: 27.74 Prob: 16.48% Token: |95|\n",
      "Top 2th token. Logit: 27.37 Prob: 11.38% Token: |99|\n",
      "Top 3th token. Logit: 27.30 Prob: 10.55% Token: |94|\n",
      "Top 4th token. Logit: 27.10 Prob:  8.65% Token: |96|\n",
      "Top 5th token. Logit: 26.75 Prob:  6.11% Token: |92|\n",
      "Top 6th token. Logit: 26.69 Prob:  5.72% Token: |91|\n",
      "Top 7th token. Logit: 26.64 Prob:  5.48% Token: |98|\n",
      "Top 8th token. Logit: 26.46 Prob:  4.57% Token: |97|\n",
      "Top 9th token. Logit: 26.35 Prob:  4.09% Token: |93|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'03'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'03'\u001b[0m, \u001b[1;36m57\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = ds.good_sentences[88] # GPT2 doesn't always perform this task correctly, only about 99% of the time.\n",
    "# On example input <|endoftext|> The pursuit lasted from the year 1290 to the year 12 , the top prediction is '90'.\n",
    "example_answer = '03'\n",
    "\n",
    "transformer_lens.utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4e0769-739d-460d-bb4e-1ced376a28d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is not a pure function. It depends on ds.good_mask, sample_idxs, and year_indices.\n",
    "def score_logits(logits, sample_idxs_0):\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits[:, -1, :], device='cpu'), dim=-1).numpy() # sad\n",
    "    probs_for_year_tokens = probs[:, year_indices.cpu().numpy()]\n",
    "    probs_for_correct_years = probs_for_year_tokens[ds.good_mask.cpu().numpy()[sample_idxs_0]]\n",
    "    correct_score = np.sum(probs_for_correct_years)\n",
    "    probs_for_incorrect_years = probs_for_year_tokens[np.logical_not(ds.good_mask.cpu().numpy()[sample_idxs_0])]\n",
    "    incorrect_score = np.sum(probs_for_incorrect_years)\n",
    "    return (correct_score - incorrect_score) / len(sample_idxs_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80318fda-1f1c-4c85-a2c4-d338ecac0bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "'''\n",
    "ranges = [\n",
    "        [layer for layer in range(num_layers)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        [attention_head_idx for attention_head_idx in range(num_attention_heads)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "'''\n",
    "ablation_sets = []\n",
    "for layer in range(num_layers):\n",
    "    for head_idx in range(num_attention_heads):\n",
    "        ablation_sets.append(tuple(Node(layer, seq_pos, head_idx) for seq_pos in range(seq_len)))\n",
    "target_nodes = []\n",
    "\n",
    "# cache activations for faster batch run\n",
    "out_decomp, _, _, pre_layer_activations = prop_GPT(ds.good_toks[sample_idxs, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(ds.good_toks[sample_idxs, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets, num_at_time=(max(64 // len(sample_idxs), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebea8bf-a588-4f3f-ad71-45c2f87f23f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_logits_decomposition_scores(out_decomps, sample_idxs, normalized=False):\n",
    "    logits = (out_decomps[0].rel + out_decomps[0].irrel)\n",
    "    full_score = score_logits(logits, sample_idxs)\n",
    "    assert(full_score > 0) # as mentioned above, GPT2 doesn't succeed at this 100% of the time\n",
    "    \n",
    "    results = []\n",
    "    relevances = np.zeros((num_layers, num_attention_heads))\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "\n",
    "        for head_idx in range(num_attention_heads):\n",
    "            decomp = out_decomps[layer_idx * num_attention_heads + head_idx]\n",
    "            score = score_logits(decomp.rel, sample_idxs)\n",
    "            norm_score = score / full_score\n",
    "            relevances[layer_idx, head_idx] = norm_score\n",
    "            if not normalized:\n",
    "                results.append(Result(decomp.ablation_set, norm_score))\n",
    "    if normalized:\n",
    "        sums_per_layer = np.sum(np.abs(relevances), axis=(1))\n",
    "        print(sums_per_layer)\n",
    "\n",
    "        sums_per_layer[sums_per_layer == 0] = -1e-8\n",
    "        relevances = relevances / np.expand_dims(sums_per_layer, (1))\n",
    "        for layer_idx in range(num_layers):\n",
    "            for head_idx in range(num_attention_heads):\n",
    "                target_decomp = target_decomps[layer_idx * num_attention_heads + head_idx]\n",
    "                results.append(Result(target_decomp.ablation_set, relevances[layer_idx, head_idx]))\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "\n",
    "    return results, relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397ecd2-9d0d-4edc-b795-de7b94484fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results, relevances = compute_logits_decomposition_scores(out_decomps, sample_idxs, normalized=True)\n",
    "\n",
    "results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "for result in results[:20]:\n",
    "    print(result.ablation_set[0], result.score)\n",
    "'''\n",
    " a9.h1, while\n",
    "MLP 8 relies on a8.h11, a8.h8, a7.h10, a6.h9, a5.h5, and a5.h1\n",
    "\n",
    "(9, 1), (8, 11), (8, 8), (7, 10), (6, 9), (5, 5), (5, 1)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2b05c-eee7-4a95-aef5-126b082ec550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "\n",
    "ablation_sets = []\n",
    "for layer in range(num_layers):\n",
    "    for head_idx in range(num_attention_heads):\n",
    "        ablation_sets.append(tuple(Node(layer, seq_pos, head_idx) for seq_pos in range(seq_len)))\n",
    "target_nodes = []\n",
    "for layer, head_idx in [(9, 1), (10, 4)]:\n",
    "    for seq_pos in range(seq_len):\n",
    "        target_nodes.append(Node(layer, seq_pos, head_idx))\n",
    "\n",
    "_, _, _, pre_layer_activations = prop_GPT(ds.good_toks[sample_idxs, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(ds.good_toks[sample_idxs, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets, num_at_time=max(64 // len(sample_idxs), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebab7b5-5f80-40b6-98f1-bccf4a16274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_target_decomposition_scores(target_decomps, normalized=False):\n",
    "    results = []\n",
    "    relevances = np.zeros((num_layers, num_attention_heads))\n",
    "    for layer_idx in range(num_layers):\n",
    "        for head_idx in range(num_attention_heads):\n",
    "            idx = layer_idx * num_attention_heads + head_idx\n",
    "            target_decomp = target_decomps[idx]\n",
    "            if target_decomp.ablation_set[0] in target_nodes:\n",
    "                continue\n",
    "            score = 0\n",
    "            for target_node_idx in range(len(target_decomp.target_nodes)):\n",
    "                for batch_idx in range(len(target_decomp.rels)):\n",
    "                    rels_magnitude = torch.mean(abs(target_decomp.rels[target_node_idx])) # np.mean if you are on cpu\n",
    "                    irrels_magnitude = torch.mean(abs(target_decomp.irrels[batch_idx])) # np.mean if you are on cpu\n",
    "                    target_node_score = rels_magnitude / (rels_magnitude + irrels_magnitude)\n",
    "                    score += target_node_score\n",
    "            if score != 0:\n",
    "                score /= len(target_decomp.rels)\n",
    "\n",
    "            relevances[layer_idx, head_idx] = score\n",
    "            if not normalized:\n",
    "                results.append(Result(target_decomp.ablation_set, relevances[layer_idx, head_idx]))\n",
    "\n",
    "\n",
    "    if normalized:\n",
    "        sums_per_layer = np.abs(np.sum(relevances, axis=(1)))\n",
    "        sums_per_layer[sums_per_layer == 0] = -1e-8\n",
    "        relevances = relevances / np.expand_dims(sums_per_layer, (1))\n",
    "\n",
    "        for layer_idx in range(num_layers):\n",
    "            for head_idx in range(num_attention_heads):\n",
    "                target_decomp = target_decomps[layer_idx * num_attention_heads + head_idx]\n",
    "                results.append(Result(target_decomp.ablation_set, relevances[layer_idx, head_idx]))\n",
    "\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "    return results, relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5db37-3c6c-407a-a7b8-86e2723c2f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results, relevances = calculate_target_decomposition_scores(target_decomps, normalized=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result.ablation_set[0], result.score)\n",
    "    # print(result)\n",
    "'''\n",
    "(9, 1), (8, 11), (8, 8), (7, 10), (6, 9), (5, 5), (5, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb0c2f-bef4-4bc3-8f5a-f2177a17e538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "for it in outliers_per_iter:\n",
    "    for result in it:\n",
    "        if result.ablation_set[0] not in all_nodes:\n",
    "            all_nodes.append(result.ablation_set[0])\n",
    "for node in all_nodes:\n",
    "    print((node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cb13f-4b1f-4bd0-9b8b-1a3bccbfe9ad",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Circuit evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33374a25-c2a8-493e-8480-ecc2274e6ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranges = [\n",
    "        [layer for layer in range(num_layers)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        # [ioi_dataset.word_idx['IO'][0]],\n",
    "        [attention_head_idx for attention_head_idx in range(num_attention_heads)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "random_circuit = random.sample(source_nodes, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe00b8-8a79-49f4-bb0d-26af18a46525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implicitly depends on year_indices/\n",
    "def correctness_rate(logits, sample_idxs_0):\n",
    "    logits_for_year_tokens = logits[:, -1, year_indices]\n",
    "    predicted_year_idxs = np.argmax(logits_for_year_tokens.cpu().numpy(), axis=-1)\n",
    "    # print(predicted_year_idxs.shape)\n",
    "    correct_per_input = ds.good_mask.cpu().numpy()[sample_idxs_0, predicted_year_idxs]\n",
    "    return np.sum(correct_per_input) / len(sample_idxs_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52058ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_circuit(circuit, full_model=False):\n",
    "    # mean_acts = mean_acts.view(old_shape)\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "\n",
    "    if full_model:\n",
    "        ablation_model = model\n",
    "    else:\n",
    "        ablation_model = add_mean_ablation_hook(model, patch_values=mean_acts.view(old_shape), circuit=circuit)\n",
    "    \n",
    "    # batching, since the datasets for this task are typically large\n",
    "    NUM_AT_TIME = 64\n",
    "    start_idx = 0\n",
    "    score = 0\n",
    "    correctness = 0\n",
    "    while True:\n",
    "        end_idx = start_idx + NUM_AT_TIME\n",
    "        if end_idx > N:\n",
    "            end_idx = N\n",
    "\n",
    "        logits, cache = model.run_with_cache(ds.good_toks[start_idx:end_idx]) # run on entire dataset along batch dimension\n",
    "        batch_score = score_logits(logits, range(start_idx, end_idx))\n",
    "        batch_correctness_rate = correctness_rate(logits, range(start_idx, end_idx))\n",
    "        num_samples = end_idx - start_idx\n",
    "        score += batch_score * (num_samples / N)\n",
    "        correctness += batch_correctness_rate * (num_samples / N)\n",
    "        start_idx += NUM_AT_TIME\n",
    "        if end_idx == N:\n",
    "            break\n",
    "    print(score)\n",
    "    print(correctness)\n",
    "    ablation_model.reset_hooks(including_permanent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8266c4-7c2b-4b14-bc5d-2698140a168b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "circuit = []\n",
    "for (layer_idx, head_idx) in [(9, 1), (8, 11), (8, 8), (7, 10), (6, 9), (5, 5), (5, 1)]: # greater-than paper's result\n",
    "    for seq_pos in range(seq_len):\n",
    "        circuit.append(Node(layer_idx, seq_pos, head_idx))\n",
    "\n",
    "evaluate_circuit(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffb5dd-88dd-4da0-b912-95b076d55c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_circuit(None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44e151-a5d9-4b23-bdfc-9da55be07bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "circuit = []\n",
    "for (layer_idx, head_idx) in [(9, 1), (10, 4), (7, 10), (11, 8), (10, 7), (6, 9), (8, 11), (8, 8)]: # the above but without seq pos\n",
    "    for seq_pos in range(seq_len):\n",
    "        circuit.append(Node(layer_idx, seq_pos, head_idx))\n",
    "evaluate_circuit(circuit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
