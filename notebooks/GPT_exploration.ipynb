{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# CD-T Imports\n",
    "import math\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson, combine_token_attn, compute_word_intervals\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.patch_hh import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6ada4-4781-4789-b3b1-1d044c11b3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x77cb334f2cf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load Model\n",
    "\n",
    "Note: Unlike with the BERT model + medical dataset objective, it is not necessary to pretrain GPT-2 to perform the IOI dataset.\n",
    "GPT-2-small is already capable of performing IOI; that's part of the point of the Mech Interp in the Wild paper.\n",
    "We only need to examine how it does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3183be1-3bf6-4f5a-8134-9bdd83db0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4838142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820d21e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37056ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(object, ()),\n",
       " [(torch.nn.modules.module.Module, (object,)),\n",
       "  [(transformers.modeling_utils.PreTrainedModel,\n",
       "    (torch.nn.modules.module.Module,\n",
       "     transformers.modeling_utils.ModuleUtilsMixin,\n",
       "     transformers.generation.utils.GenerationMixin,\n",
       "     transformers.utils.hub.PushToHubMixin,\n",
       "     transformers.integrations.peft.PeftAdapterMixin)),\n",
       "   [(transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       "     (transformers.modeling_utils.PreTrainedModel,)),\n",
       "    [(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,\n",
       "      (transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,))]]],\n",
       "  (transformers.generation.utils.GenerationMixin, (object,)),\n",
       "  [(transformers.modeling_utils.PreTrainedModel,\n",
       "    (torch.nn.modules.module.Module,\n",
       "     transformers.modeling_utils.ModuleUtilsMixin,\n",
       "     transformers.generation.utils.GenerationMixin,\n",
       "     transformers.utils.hub.PushToHubMixin,\n",
       "     transformers.integrations.peft.PeftAdapterMixin)),\n",
       "   [(transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       "     (transformers.modeling_utils.PreTrainedModel,)),\n",
       "    [(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,\n",
       "      (transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,))]]],\n",
       "  (transformers.integrations.peft.PeftAdapterMixin, (object,)),\n",
       "  [(transformers.modeling_utils.PreTrainedModel,\n",
       "    (torch.nn.modules.module.Module,\n",
       "     transformers.modeling_utils.ModuleUtilsMixin,\n",
       "     transformers.generation.utils.GenerationMixin,\n",
       "     transformers.utils.hub.PushToHubMixin,\n",
       "     transformers.integrations.peft.PeftAdapterMixin)),\n",
       "   [(transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       "     (transformers.modeling_utils.PreTrainedModel,)),\n",
       "    [(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,\n",
       "      (transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,))]]],\n",
       "  (transformers.modeling_utils.ModuleUtilsMixin, (object,)),\n",
       "  [(transformers.modeling_utils.PreTrainedModel,\n",
       "    (torch.nn.modules.module.Module,\n",
       "     transformers.modeling_utils.ModuleUtilsMixin,\n",
       "     transformers.generation.utils.GenerationMixin,\n",
       "     transformers.utils.hub.PushToHubMixin,\n",
       "     transformers.integrations.peft.PeftAdapterMixin)),\n",
       "   [(transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       "     (transformers.modeling_utils.PreTrainedModel,)),\n",
       "    [(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,\n",
       "      (transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,))]]],\n",
       "  (transformers.utils.hub.PushToHubMixin, (object,)),\n",
       "  [(transformers.modeling_utils.PreTrainedModel,\n",
       "    (torch.nn.modules.module.Module,\n",
       "     transformers.modeling_utils.ModuleUtilsMixin,\n",
       "     transformers.generation.utils.GenerationMixin,\n",
       "     transformers.utils.hub.PushToHubMixin,\n",
       "     transformers.integrations.peft.PeftAdapterMixin)),\n",
       "   [(transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       "     (transformers.modeling_utils.PreTrainedModel,)),\n",
       "    [(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,\n",
       "      (transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,))]]]]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "# inspect.getclasstree(inspect.getmro(type(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5069ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13,  198,  198,\n",
      "           40, 1101,  407, 1654,  611,  345,  821, 3910]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgiazhou/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Replace me by any text you'd like.\\n\\nI'm not sure if you're aware\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd\"\n",
    "input = tokenizer(text, return_tensors='pt').input_ids\n",
    "# print(encoded_input) # has 'input_idx' and 'attention_mask'\n",
    "# output = model(input)\n",
    "# print(output.last_hidden_state.shape)\n",
    "gen_tokens = model.generate(input, pad_token_id=tokenizer.pad_token_id, output_scores=True)\n",
    "print(gen_tokens)\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "712c94e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPastAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# other exploratory stuff\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model.to_tokens(text) #turns out this is a utility of trasnformer_lens\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(output.past_key_values[0][0].shape) # this has to do with key matrix stuff\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(output.values())\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m output\u001b[38;5;241m.\u001b[39mlogits\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPastAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "# other exploratory stuff\n",
    "# model.to_tokens(text) #turns out this is a utility of trasnformer_lens\n",
    "# print(output.past_key_values[0][0].shape) # this has to do with key matrix stuff\n",
    "#print(output.values())\n",
    "#output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b375528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m320.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (0.24.1)\n",
      "Requirement already satisfied: packaging in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.4/392.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.4 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08241bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 3.46k/3.46k [00:00<00:00, 10.5MB/s]\n",
      "Downloading data: 100%|██████████| 5.84M/5.84M [00:01<00:00, 4.59MB/s]\n",
      "Downloading data: 100%|██████████| 756M/756M [01:27<00:00, 8.65MB/s] \n",
      "Generating train split: 100%|██████████| 26210000/26210000 [00:03<00:00, 7827513.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ioi_dataset = load_dataset(\"fahamu/ioi\")\n",
    "# i've decided against using this for the most part; it's better to use the raw IOIDataset \n",
    "# from the paper and from the related notebook using EasyTransformers, since these both provide many utilities for dealing with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c0e1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(5228) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformer_lens in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.33.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.20.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.33)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (13.7.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.66.4)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.43.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.11.0)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.17.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.24.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.9.5)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.15.1)\n",
      "Requirement already satisfied: sympy in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.27.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.12.0)\n",
      "Requirement already satisfied: setproctitle in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(5229) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformer_lens\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a520f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "# This makes some sense, since EasyTransformer, the repo/lib released by the IOI guys, was forked from TransformerLens.\n",
    "# In fact, this makes the reproduction a little bit more faithful, since they most likely do certain things such as \n",
    "# \"folding\" LayerNorms to improve their interpretability results, and we are able to do the same by using TransformerLens.\n",
    "# HuggingFace, by contrast, has the most impenetrable docs and tons of outdated APIs and etc.; even their source \n",
    "# code is impossible to traverse, and I gave up on it, thankfully quickly.\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ff20c",
   "metadata": {},
   "source": [
    "### Verify forward pass\n",
    "Because the print of the model is a little messy because of hooks, and for whatever reason the model doesn't expose all of its\n",
    "intermediate activations, we do this to ensure that the model's architecture is what we think it is.\n",
    "(There are some complications with the layernorm folding that transformer_lens, and therefore the IOI paper, use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, \"cpu\")\n",
    "embedding_output = model.embed(encoding.input_ids)\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "# actually, not finishing this until it's clear that it's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9aaa465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "515c528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# print(model.config) # doesn't work on hookedtransformer, is a huggingface thing\n",
    "# print(model.embed.dtype) same, but can use dtype trick\n",
    "# print(type(model))\n",
    "#model.state_dict().keys()#.blocks[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c19f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (1.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437d3a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/var/folders/c9/mvmx0pt17m51nyh8wgjnqj900000gn/T/ipykernel_11966/1150960534.py\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      6 \u001b[0;31m\u001b[0;31m# embedding_output = model.embed(encoding.input_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "Moving model to device:  cpu\n",
      "> \u001b[0;32m/Users/georgiazhou/miniconda3/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py\u001b[0m(332)\u001b[0;36minput_to_embed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    330 \u001b[0;31m            \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    331 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 332 \u001b[0;31m        \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    333 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"standard\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    334 \u001b[0;31m            pos_embed = self.hook_pos_embed(\n",
      "\u001b[0m\n",
      "tensor([[0.4899, 0.2857, 0.5669, 0.1953, 0.6854, 0.5814, 0.6580, 0.0523, 0.2990,\n",
      "         0.1988, 0.7593, 0.1852, 0.7425, 0.4648, 0.5173, 0.7029, 0.3675, 0.9153,\n",
      "         0.7823, 0.3810, 0.3304, 0.9647, 0.0897, 0.2235, 0.4690, 0.2287, 0.8431,\n",
      "         0.8483, 0.9326, 0.9771, 0.5403, 0.3954, 0.6604, 0.7142, 0.8972, 0.2977,\n",
      "         0.7195, 0.7162, 0.3847, 0.4517, 0.9752, 0.9666, 0.0712, 0.0250, 0.8160,\n",
      "         0.3575, 0.9512, 0.7769, 0.2854, 0.4061, 0.7344, 0.1340, 0.7894, 0.9840,\n",
      "         0.1162, 0.6278, 0.0486, 0.9728, 0.3930, 0.8698, 0.5723, 0.4823, 0.1816,\n",
      "         0.7587, 0.4184, 0.6558, 0.1571, 0.4515, 0.4165, 0.8522, 0.8111, 0.6924,\n",
      "         0.6437, 0.9026, 0.2192, 0.0989, 0.5159, 0.3355, 0.8494, 0.6002, 0.7349,\n",
      "         0.0083, 0.9187, 0.0925, 0.8987, 0.7683, 0.3159, 0.0378, 0.1252, 0.5801,\n",
      "         0.0203, 0.0947, 0.3744, 0.7115, 0.3326, 0.3328, 0.6923, 0.2880, 0.9191,\n",
      "         0.8675, 0.7663, 0.8713, 0.7144, 0.4411, 0.6582, 0.0898, 0.4590, 0.3271,\n",
      "         0.2631, 0.0537, 0.6148, 0.3446, 0.5498, 0.8089, 0.9017, 0.4061, 0.0830,\n",
      "         0.8876, 0.1901, 0.7061, 0.0136, 0.1473, 0.9664, 0.9490, 0.1113, 0.1925,\n",
      "         0.8839, 0.5137, 0.5024, 0.0486, 0.4943, 0.2920, 0.9127, 0.0786, 0.4882,\n",
      "         0.8552, 0.6846, 0.4457, 0.5783, 0.5699, 0.6590, 0.3101, 0.1902, 0.7043,\n",
      "         0.3915, 0.0648, 0.0240, 0.3804, 0.1248, 0.1597, 0.0679, 0.3468, 0.6730,\n",
      "         0.8988, 0.4014, 0.2396, 0.2250, 0.5709, 0.3526, 0.1530, 0.0991, 0.5143,\n",
      "         0.6921, 0.0903, 0.6905, 0.1902, 0.0551, 0.5591, 0.4701, 0.6137, 0.5072,\n",
      "         0.3103, 0.8234, 0.9188, 0.2785, 0.2089, 0.5823, 0.4982, 0.4530, 0.5238,\n",
      "         0.0391, 0.3277, 0.6176, 0.8468, 0.4664, 0.5836, 0.1416, 0.0454, 0.8716,\n",
      "         0.3649, 0.9487, 0.1167, 0.7138, 0.0287, 0.1598, 0.5213, 0.2092, 0.9240,\n",
      "         0.4404, 0.4938, 0.8985, 0.2073, 0.7810, 0.3395, 0.1523, 0.0434, 0.9561,\n",
      "         0.9232, 0.8999, 0.0324, 0.0425, 0.3745, 0.6305, 0.7821, 0.9206, 0.5554,\n",
      "         0.2627, 0.7650, 0.9858, 0.1061, 0.9053, 0.1728, 0.0382, 0.1807, 0.6976,\n",
      "         0.5316, 0.5923, 0.4144, 0.4891, 0.1001, 0.1540, 0.3216, 0.3637, 0.7163,\n",
      "         0.8862, 0.4387, 0.5797, 0.8650, 0.1084, 0.5112, 0.8885, 0.3980, 0.0343,\n",
      "         0.0434, 0.1546, 0.5567, 0.7556, 0.3330, 0.5119, 0.6388, 0.7013, 0.8565,\n",
      "         0.5847, 0.5807, 0.5563, 0.5279, 0.3003, 0.3859, 0.4216, 0.5219, 0.1395,\n",
      "         0.2036, 0.9654, 0.9772, 0.7715, 0.2956, 0.2767, 0.9410, 0.3786, 0.0932,\n",
      "         0.3292, 0.5029, 0.5760, 0.0208, 0.0637, 0.9186, 0.9885, 0.7412, 0.1572,\n",
      "         0.5471, 0.3899, 0.8888, 0.9664, 0.0497, 0.9365, 0.2104, 0.8621, 0.7478,\n",
      "         0.6707, 0.8473, 0.7479, 0.5224, 0.9571, 0.4775, 0.3613, 0.3901, 0.8054,\n",
      "         0.4893, 0.8736, 0.3289, 0.0871, 0.5696, 0.3937, 0.7633, 0.8726, 0.6764,\n",
      "         0.0977, 0.0907, 0.2916, 0.7737, 0.9670, 0.0652, 0.6116, 0.6232, 0.2342,\n",
      "         0.6466, 0.4742, 0.2253, 0.0282, 0.1587, 0.3295, 0.8226, 0.6524, 0.7975,\n",
      "         0.4274, 0.1326, 0.1188, 0.4811, 0.5123, 0.4046, 0.5831, 0.7454, 0.7263,\n",
      "         0.0485, 0.3007, 0.9539, 0.5549, 0.0363, 0.1347, 0.6428, 0.8186, 0.8706,\n",
      "         0.3836, 0.9726, 0.9615, 0.1727, 0.7548, 0.4441, 0.7454, 0.6260, 0.7024,\n",
      "         0.1654, 0.8457, 0.7552, 0.4047, 0.6426, 0.9379, 0.5923, 0.9384, 0.3322,\n",
      "         0.1395, 0.1710, 0.6311, 0.0645, 0.1388, 0.8102, 0.8372, 0.3399, 0.2006,\n",
      "         0.8864, 0.6795, 0.4333, 0.0924, 0.9547, 0.7047, 0.7579, 0.3229, 0.0173,\n",
      "         0.5777, 0.5384, 0.0618, 0.0246, 0.5326, 0.5614, 0.3867, 0.6161, 0.8893,\n",
      "         0.0152, 0.6122, 0.4878, 0.2037, 0.8578, 0.2855, 0.9086, 0.9782, 0.1208,\n",
      "         0.2376, 0.7118, 0.6967, 0.5760, 0.3038, 0.4250, 0.3302, 0.5346, 0.6359,\n",
      "         0.6297, 0.5526, 0.9838, 0.7923, 0.4786, 0.1191, 0.2552, 0.6948, 0.6776,\n",
      "         0.2140, 0.3437, 0.7803, 0.7786, 0.4308, 0.5289, 0.8695, 0.3809, 0.6609,\n",
      "         0.8122, 0.7698, 0.3418, 0.3925, 0.1280, 0.4924, 0.2541, 0.3799, 0.6441,\n",
      "         0.5129, 0.3139, 0.7623, 0.7880, 0.9771, 0.7432, 0.1450, 0.3149, 0.0335,\n",
      "         0.8724, 0.8111, 0.9561, 0.2345, 0.6864, 0.4894, 0.4488, 0.9238, 0.3203,\n",
      "         0.4466, 0.5934, 0.9474, 0.6028, 0.3135, 0.8827, 0.5949, 0.7135, 0.9596,\n",
      "         0.9333, 0.2618, 0.0168, 0.5854, 0.9003, 0.0157, 0.1732, 0.7766, 0.1983,\n",
      "         0.0749, 0.2900, 0.4583, 0.5228, 0.6954, 0.2390, 0.9702, 0.4439, 0.3731,\n",
      "         0.0070, 0.4164, 0.0287, 0.9959, 0.9893, 0.2160, 0.7416, 0.4318, 0.3783,\n",
      "         0.5098, 0.0504, 0.2660, 0.7073, 0.9229, 0.1133, 0.3662, 0.3290, 0.2825,\n",
      "         0.9583, 0.6319, 0.4753, 0.0157, 0.9813, 0.5651, 0.0764, 0.5601, 0.0528,\n",
      "         0.3889, 0.5240, 0.6499, 0.8602, 0.1737, 0.2780, 0.7497, 0.2339]])\n",
      "torch.Size([1, 512])\n",
      "*** IndexError: tensors used as indices must be long, int, byte or bool tensors\n",
      "*** IndexError: tensors used as indices must be long, int, byte or bool tensors\n",
      "*** IndexError: tensors used as indices must be long, int, byte or bool tensors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:524\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     (\n\u001b[1;32m    520\u001b[0m         residual,\n\u001b[1;32m    521\u001b[0m         tokens,\n\u001b[1;32m    522\u001b[0m         shortformer_pos_embed,\n\u001b[1;32m    523\u001b[0m         attention_mask,\n\u001b[0;32m--> 524\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_to_embed(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    526\u001b[0m         prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos,\n\u001b[1;32m    527\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m    528\u001b[0m         past_kv_cache\u001b[38;5;241m=\u001b[39mpast_kv_cache,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:332\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    331\u001b[0m pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m--> 332\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_embed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(tokens))  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/bdb.py:90\u001b[0m, in \u001b[0;36mBdb.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_line(frame)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/bdb.py:115\u001b[0m, in \u001b[0;36mBdb.dispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_line(frame)\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquitting: \u001b[38;5;28;01mraise\u001b[39;00m BdbQuit\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n",
      "\u001b[0;31mBdbQuit\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_shape)\n\u001b[1;32m      9\u001b[0m pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m---> 10\u001b[0m summary(model, input_shape, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "from torchinfo import summary\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, \"cpu\")\n",
    "# embedding_output = model.embed(encoding.input_ids)\n",
    "input_shape = encoding.input_ids.shape\n",
    "print(input_shape)\n",
    "pdb.set_trace()\n",
    "summary(model, input_shape, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8823997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same as in the notebook, example\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \"Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377977-fe3b-45dd-9d00-1c19e5366038",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generate dataset/Explore types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b295089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 4, 9, 6, 1, 13, 2, 8, 8, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = IOIDataset(N=500, prompt_type=\"ABBA\", tokenizer=model.tokenizer)\n",
    "#data.tokenized_prompts\n",
    "data.ioi_prompts[0]\n",
    "[x['TEMPLATE_IDX'] for x in data.ioi_prompts[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92798f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Attention' object has no attribute 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m get_encoding(text, model\u001b[38;5;241m.\u001b[39mtokenizer, device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# encoding.input_ids.shape # 512-long vector, not sure why the tokens change from EOS to 0 at some point\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# embedding = model.embed(encoding.input_ids)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m out_decomps, target_decomps \u001b[38;5;241m=\u001b[39m prop_model_hh_batched(encoding, model, source_list, target_nodes,\n\u001b[1;32m     17\u001b[0m                                                                    device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     18\u001b[0m                                                                    patched_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mean_ablated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/research-machine1/CD_Circuit/pyfunctions/patch_hh.py:319\u001b[0m, in \u001b[0;36mprop_model_hh_batched\u001b[0;34m(encoding, model, source_node_list, target_nodes, device, patched_values, num_at_time, n_layers, att_list, output_att_prob, mean_ablated)\u001b[0m\n\u001b[1;32m    311\u001b[0m     layer_out_decomps, layer_target_decomps, att_probs_lst \u001b[38;5;241m=\u001b[39m prop_BERT_hh(encoding, model, \n\u001b[1;32m    312\u001b[0m                                                                     source_node_list[b_st: b_end],\n\u001b[1;32m    313\u001b[0m                                                                     target_nodes, device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                                                     output_att_prob\u001b[38;5;241m=\u001b[39moutput_att_prob,\n\u001b[1;32m    317\u001b[0m                                                                     mean_ablated\u001b[38;5;241m=\u001b[39mmean_ablated)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, transformer_lens\u001b[38;5;241m.\u001b[39mHookedTransformer):\n\u001b[0;32m--> 319\u001b[0m     layer_out_decomps, layer_target_decomps, att_probs_lst \u001b[38;5;241m=\u001b[39m prop_GPT_hh(encoding, model, \n\u001b[1;32m    320\u001b[0m                                                                     source_node_list[b_st: b_end],\n\u001b[1;32m    321\u001b[0m                                                                     target_nodes, device,\n\u001b[1;32m    322\u001b[0m                                                                     patched_values,\n\u001b[1;32m    323\u001b[0m                                                                     att_list\u001b[38;5;241m=\u001b[39matt_list,\n\u001b[1;32m    324\u001b[0m                                                                     output_att_prob\u001b[38;5;241m=\u001b[39moutput_att_prob,\n\u001b[1;32m    325\u001b[0m                                                                     mean_ablated\u001b[38;5;241m=\u001b[39mmean_ablated)\n\u001b[1;32m    327\u001b[0m out_decomps \u001b[38;5;241m=\u001b[39m out_decomps \u001b[38;5;241m+\u001b[39m layer_out_decomps\n\u001b[1;32m    328\u001b[0m target_decomps \u001b[38;5;241m=\u001b[39m [target_decomps[i] \u001b[38;5;241m+\u001b[39m layer_target_decomps[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers)]\n",
      "File \u001b[0;32m~/research-machine1/CD_Circuit/pyfunctions/patch_hh.py:266\u001b[0m, in \u001b[0;36mprop_GPT_hh\u001b[0;34m(encoding, model, source_node_list, target_nodes, device, patched_values, att_list, output_att_prob, mean_ablated)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     layer_patched_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m rel_n, irrel_n, layer_target_decomps, returned_att_probs \u001b[38;5;241m=\u001b[39m prop_layer_hh(rel, irrel, extended_attention_mask, \n\u001b[1;32m    267\u001b[0m                                                                          layer_head_mask, source_node_list, \n\u001b[1;32m    268\u001b[0m                                                                          target_nodes, i, \n\u001b[1;32m    269\u001b[0m                                                                          layer_patched_values,\n\u001b[1;32m    270\u001b[0m                                                                          layer_module, \n\u001b[1;32m    271\u001b[0m                                                                          device,\n\u001b[1;32m    272\u001b[0m                                                                          att_probs, output_att_prob,\n\u001b[1;32m    273\u001b[0m                                                                          mean_ablated\u001b[38;5;241m=\u001b[39mmean_ablated,\n\u001b[1;32m    274\u001b[0m                                                                          attention_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    275\u001b[0m target_decomps\u001b[38;5;241m.\u001b[39mappend(layer_target_decomps)\n\u001b[1;32m    276\u001b[0m normalize_rel_irrel(rel_n, irrel_n)\n",
      "File \u001b[0;32m~/research-machine1/CD_Circuit/pyfunctions/patch_hh.py:146\u001b[0m, in \u001b[0;36mprop_layer_hh\u001b[0;34m(rel, irrel, attention_mask, head_mask, source_node_list, target_nodes, level, layer_patched_values, layer_module, device, att_probs, output_att_prob, mean_ablated, attention_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprop_layer_hh\u001b[39m(rel, irrel, attention_mask, head_mask, \n\u001b[1;32m    143\u001b[0m                   source_node_list, target_nodes, level, layer_patched_values,\n\u001b[1;32m    144\u001b[0m                   layer_module, device, att_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_att_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mean_ablated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, attention_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     rel_a, irrel_a, target_decomps, returned_att_probs \u001b[38;5;241m=\u001b[39m prop_attention_hh(rel, irrel, attention_mask, \n\u001b[1;32m    147\u001b[0m                                                                            head_mask, source_node_list, \n\u001b[1;32m    148\u001b[0m                                                                            target_nodes, level, layer_patched_values,\n\u001b[1;32m    149\u001b[0m                                                                            \u001b[38;5;28mgetattr\u001b[39m(layer_module, attention_name),\n\u001b[1;32m    150\u001b[0m                                                                            device,\n\u001b[1;32m    151\u001b[0m                                                                            att_probs, output_att_prob, mean_ablated\u001b[38;5;241m=\u001b[39mmean_ablated)\n\u001b[1;32m    153\u001b[0m     i_module \u001b[38;5;241m=\u001b[39m layer_module\u001b[38;5;241m.\u001b[39mintermediate\n\u001b[1;32m    154\u001b[0m     rel_id, irrel_id \u001b[38;5;241m=\u001b[39m prop_linear(rel_a, irrel_a, i_module\u001b[38;5;241m.\u001b[39mdense)\n",
      "File \u001b[0;32m~/research-machine1/CD_Circuit/pyfunctions/patch_hh.py:111\u001b[0m, in \u001b[0;36mprop_attention_hh\u001b[0;34m(rel, irrel, attention_mask, head_mask, source_node_list, target_nodes, level, layer_patched_values, a_module, device, att_probs, output_att_prob, mean_ablated)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprop_attention_hh\u001b[39m(rel, irrel, attention_mask, \n\u001b[1;32m    101\u001b[0m                       head_mask, source_node_list, target_nodes, level,\n\u001b[1;32m    102\u001b[0m                       layer_patched_values,\n\u001b[1;32m    103\u001b[0m                       a_module, device, att_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_att_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mean_ablated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m     rel_context, irrel_context, returned_att_probs \u001b[38;5;241m=\u001b[39m prop_self_attention_hh(rel, irrel, \n\u001b[1;32m    106\u001b[0m                                                                         attention_mask, \n\u001b[1;32m    107\u001b[0m                                                                         head_mask, \n\u001b[1;32m    108\u001b[0m                                                                         source_node_list,\n\u001b[1;32m    109\u001b[0m                                                                         target_nodes,\n\u001b[1;32m    110\u001b[0m                                                                         level,\n\u001b[0;32m--> 111\u001b[0m                                                                         a_module\u001b[38;5;241m.\u001b[39mself,\n\u001b[1;32m    112\u001b[0m                                                                         device,\n\u001b[1;32m    113\u001b[0m                                                                         att_probs,\n\u001b[1;32m    114\u001b[0m                                                                         output_att_prob\u001b[38;5;241m=\u001b[39moutput_att_prob)\n\u001b[1;32m    115\u001b[0m     normalize_rel_irrel(rel_context, irrel_context)\n\u001b[1;32m    117\u001b[0m     output_module \u001b[38;5;241m=\u001b[39m a_module\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Attention' object has no attribute 'self'"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pos_specific_hs = [\n",
    "        [i for i in range(12)],\n",
    "        [0],\n",
    "        [i for i in range(12)]\n",
    "    ]\n",
    "all_heads = list(itertools.product(*pos_specific_hs))\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)] # not meaningful in a GPT context\n",
    "source_list = [[node] for node in all_heads if node not in target_nodes]\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "# encoding.input_ids.shape # 512-long vector, not sure why the tokens change from EOS to 0 at some point\n",
    "# embedding = model.embed(encoding.input_ids)\n",
    "\n",
    "out_decomps, target_decomps = prop_model_hh_batched(encoding, model, source_list, target_nodes,\n",
    "                                                                   device=device,\n",
    "                                                                   patched_values=None, mean_ablated=False)\n",
    "                                                                   # patched_values=mean_act, mean_ablated=True)\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43bc3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76277da4-2c38-4e47-8c4a-1c5b462943b7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Head to head direct influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693cd0e8-2fd3-4e60-bb2f-00efa1356dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_hh_at_pos(encoding, model, target_nodes, pos=0, mean_act=None, mean_ablated=False):\n",
    "    pos_specific_hs = [\n",
    "        [i for i in range(12)],\n",
    "        [pos],\n",
    "        [i for i in range(12)]\n",
    "    ]\n",
    "    all_heads = list(itertools.product(*pos_specific_hs))\n",
    "\n",
    "    # patch one node at a time\n",
    "    h_ctbn_list = []\n",
    "    \n",
    "    source_list = [[node] for node in all_heads if node not in target_nodes]\n",
    "    out_decomps, target_decomps = prop_classifier_model_hh_batched(encoding, model, source_list, target_nodes,\n",
    "                                                                   device=device,\n",
    "                                                                   patched_values=mean_act, mean_ablated=True)\n",
    "    for i, _ in enumerate(source_list):\n",
    "        ctbn = 0\n",
    "        for l in range(12):\n",
    "            if target_decomps[l][i][0].shape[0] != 0:\n",
    "                rel_part = np.mean(abs(target_decomps[l][i][0]))\n",
    "                irrel_part = np.mean(abs(target_decomps[l][i][1]))\n",
    "                ctbn += rel_part / abs(rel_part + irrel_part) * 100\n",
    "        h_ctbn_list.append(ctbn)\n",
    "        \n",
    "    return source_list, h_ctbn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff0fe2e-2ba8-4470-9dcd-7c65b048ccfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform on one doc as an example\n",
    "text = documents_full[0]\n",
    "label = labels_full[0]\n",
    "encoding = get_encoding(text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd8d1919-0e91-4f59-a07d-05847645d6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [1:52:20<00:00, 13.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# perform one iteration measuring effect of the source nodes to target nodes as an example\n",
    "# note that target nodes get updated in each iteration\n",
    "\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "\n",
    "all_source_hs = []\n",
    "all_htbn = []\n",
    "for pos in tqdm.tqdm(range(512)):\n",
    "    with torch.no_grad():\n",
    "        source_list, h_ctbn_list = patch_hh_at_pos(encoding, model, target_nodes, pos=pos, mean_act=back, mean_ablated=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    all_source_hs.append(source_list)\n",
    "    all_htbn.append(h_ctbn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e6c0bf3b-0617-4884-80d7-8091ddedce64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_ctbn = [c for sublist in all_htbn for c in sublist]\n",
    "flat_source_h = [c for sublist in all_source_hs for c in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d0e92299-54f4-4428-bf98-b920b237831e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_idx = sorted(range(len(flat_ctbn)), key=lambda i: flat_ctbn[i])[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d48ead3-05fc-43d4-b90f-a32d822bc2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 82, 4)] 36.659424751996994\n",
      "[(5, 82, 4)] 42.90880411863327\n",
      "[(3, 82, 0)] 51.443591713905334\n",
      "[(4, 82, 0)] 87.46089041233063\n",
      "[(5, 82, 0)] 103.08798849582672\n",
      "[(6, 82, 0)] 132.23715126514435\n"
     ]
    }
   ],
   "source": [
    "for i in top_idx:\n",
    "    print(flat_source_h[i], flat_ctbn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "98bc384b-e405-4815-b919-3b6d03739e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the identified heads\n",
    "path = f\"{base_dir}/output/{args['task']}/{args['model_type']}_{args['field']}/h3\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(path, f\"flat_source_h.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(flat_source_h, handle)\n",
    "    \n",
    "with open(os.path.join(path, f\"flat_source_h.pkl\"), 'rb') as handle:\n",
    "    back = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515c210-6040-41c2-a3a5-9d55d7112863",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Examine the attended words by the identified heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1bf9c5-979b-4473-8958-e34adab200a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_attended_tokens_hh(positives_heads, device, tokenizer, N=100, Z_thres=2, percentile=75, use_perc=False):\n",
    "    index_lst = random.sample(range(0, len(documents_full)), N)\n",
    "    docs = [documents_full[i] for i in index_lst]\n",
    "    \n",
    "    collect = collections.defaultdict(int)\n",
    "    for doc in docs:\n",
    "        encoding = get_encoding(doc, tokenizer, device)\n",
    "        \n",
    "        _, _, raw_att_probs_lst = prop_classifier_model_hh(encoding, model, [[]], [], device=device, output_att_prob=True)\n",
    "        raw_att_probs = torch.stack(raw_att_probs_lst).cpu().numpy()\n",
    "\n",
    "        avg_att_m = np.zeros((512))\n",
    "        for level, pos, h in positives_heads:\n",
    "            att_m = raw_att_probs[level, h, pos, :]\n",
    "            avg_att_m += att_m\n",
    "\n",
    "        avg_att_m /= len(positives)\n",
    "        \n",
    "        # convert to word level\n",
    "        interval_dict, word_lst = compute_word_intervals(encoding, tokenizer)\n",
    "        word_att_m = combine_token_attn(interval_dict, avg_att_m)\n",
    "        \n",
    "        if use_perc:\n",
    "            perc_cutoff = np.percentile(word_att_m, percentile)\n",
    "            positive_words = np.where(word_att_m > perc_cutoff)\n",
    "        else:\n",
    "            Z = (word_att_m - np.mean(word_att_m)) / np.std(word_att_m)\n",
    "            positive_words = np.where(Z > Z_thres)\n",
    "        \n",
    "        for w_idx in positive_words[0]:\n",
    "            w = word_lst[w_idx]\n",
    "            #collect[w] += 1\n",
    "            collect[w] += word_att_m[w_idx]\n",
    "            \n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b2bbf39-9363-45ae-b516-2da5e1d9d773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_attended_tokens_hh_rm_pos(positives_heads, device, tokenizer, N=100, Z_thres=2, percentile=75, use_perc=False):\n",
    "    index_lst = random.sample(range(0, len(documents_full)), N)\n",
    "    docs = [documents_full[i] for i in index_lst]\n",
    "    \n",
    "    collect = collections.defaultdict(int)\n",
    "    for doc in docs:\n",
    "        encoding = get_encoding(doc, tokenizer, device)\n",
    "        \n",
    "        _, _, raw_att_probs_lst = prop_classifier_model_hh(encoding, model, [[]], [], device=device, output_att_prob=True)\n",
    "        raw_att_probs = torch.stack(raw_att_probs_lst).cpu().numpy()\n",
    "\n",
    "        avg_att_m = np.zeros((512))\n",
    "        for level, _, h in positives_heads:\n",
    "            att_m = raw_att_probs[level, h, :, :]\n",
    "            #att_m = np.mean(att_m, axis=0)\n",
    "            max_row = np.unravel_index(np.argmax(att_m, axis=None), att_m.shape)[0]\n",
    "            avg_att_m += att_m[max_row, :]\n",
    "\n",
    "        avg_att_m /= len(positives)\n",
    "        \n",
    "        # convert to word level\n",
    "        interval_dict, word_lst = compute_word_intervals(encoding, tokenizer)\n",
    "        word_att_m = combine_token_attn(interval_dict, avg_att_m)\n",
    "        \n",
    "        if use_perc:\n",
    "            perc_cutoff = np.percentile(word_att_m, percentile)\n",
    "            positive_words = np.where(word_att_m > perc_cutoff)\n",
    "        else:\n",
    "            Z = (word_att_m - np.mean(word_att_m)) / np.std(word_att_m)\n",
    "            positive_words = np.where(Z > Z_thres)\n",
    "        \n",
    "        for w_idx in positive_words[0]:\n",
    "            w = word_lst[w_idx]\n",
    "            #collect[w] += 1\n",
    "            collect[w] += word_att_m[w_idx]\n",
    "            \n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfd9dacb-2fb8-40e3-bdd1-94a1f62b1973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negatives = [(11, 2), (11, 5), (11, 6), (11, 9), (11, 10), (11, 11),\n",
    "             (10, 3), (10, 4), (10, 5), (10, 6), (10, 9), (10, 10), (10, 11),\n",
    "             (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 8), (9, 9), (9, 10), (9, 11),\n",
    "             (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11),\n",
    "             (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 7), (7, 8), (7, 9), (7, 10),\n",
    "             (6, 1), (6, 2), (6, 3), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11),\n",
    "             (5, 1), (5, 2), (5, 3), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11),\n",
    "             (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11),\n",
    "             (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11),\n",
    "             (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11),\n",
    "             (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 0), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11),\n",
    "             (0, 0), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 8), (0, 10), (0, 11),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36cebd9e-5d02-41cc-923b-65e88bf24b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the identified attn heads using CD-T\n",
    "pos_specific_hs = [\n",
    "            [i for i in range(12)],\n",
    "            [i for i in range(512)],\n",
    "            [i for i in range(12)]\n",
    "        ]\n",
    "all_heads = list(itertools.product(*pos_specific_hs))\n",
    "random_heads = random.sample(all_heads, 6)\n",
    "positives = random_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87528ebc-415b-45f6-a6bc-5cdf193d482b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positives = [(1, 169, 2), (2, 169, 2), (2, 169, 3), (4, 169, 8), (1, 411, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc7df4ef-9292-4b83-8201-2ad57cfee8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_attended_token_freq = collect_attended_tokens_hh_rm_pos(positives, device, tokenizer, N=200, use_perc=True)\n",
    "positive_attended_token_freq = sorted(positive_attended_token_freq.items(), key=lambda k_v: k_v[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42c0c0cc-0f63-466d-b586-822bbca6e8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#h1 = [(10, 82, 0), (10, 61, 8), (10, 82, 7), (10, 176, 2), (10, 467, 1), (10, 91, 7)]\n",
    "#h2 = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "#h3 = [(6, 82, 4), (5, 82, 4), (3, 82, 0), (4, 82, 0), (5, 82, 0), (6, 82, 0)]\n",
    "#positives = [(0, 82, 9), (0, 82, 1), (0, 82, 7), (1, 82, 6), (0, 82, 6), (2, 82, 0)]\n",
    "positive_attended_token_freq = collect_attended_tokens_hh(positives, device, tokenizer, N=500, use_perc=True)\n",
    "positive_attended_token_freq = sorted(positive_attended_token_freq.items(), key=lambda k_v: k_v[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5fb143fa-8d8d-4185-8931-53d94c329144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pp15_h2.json', 'w') as fp:\n",
    "    json.dump(positive_attended_token_freq, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb563fe-5586-4600-a700-a5fe4c23d87d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4494d04-0b59-4e12-ae4f-660c1680398e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = documents_full[0]\n",
    "label = labels_full[0]\n",
    "encoding = get_encoding(text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ccb2c31-7d29-4307-8309-76454da052ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "source_list_30 = [#list(itertools.product(range(12), range(512), range(12))), \n",
    "                  # list(itertools.product(range(12), range(70, 85), range(12))), \n",
    "                  # [(11, 0, i) for i in range(12)]\n",
    "                  [(0, 0, 0)]] * 30\n",
    "source_list_60 = [#list(itertools.product(range(12), range(512), range(12))), \n",
    "                  # list(itertools.product(range(12), range(70, 85), range(12))), \n",
    "                  # [(11, 0, i) for i in range(12)]\n",
    "                  [(0, 0, 0)], []] * 30\n",
    "\"\"\"\n",
    "target_nodes = [(11, 8), (11, 0), (11, 1), (11, 4), (11, 3), (11, 7)]\n",
    "source_list = [[(5, 7, 0)], [(5, 5, 0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140cd70-be4d-43ac-a2dc-fd62f8e9d73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_list = [[(5, 7, 0)], [(5, 5, 0)]]\n",
    "out_decomps, target_decomps = prop_classifier_model_hh_batched(encoding, model, source_list, target_nodes, patched_values=back, mean_ablated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e27d0-769c-4c06-9114-e567034a1372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_decomps, target_decomps, _ = prop_classifier_model_hh(encoding, model, source_list, target_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cdcc84d2-f700-4b3b-a62d-a397b657dbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([-0.0171,  0.0302, -0.0149], device='cuda:0'),\n",
       "  tensor([-3.2685,  6.1339, -3.2953], device='cuda:0')),\n",
       " (tensor([-0.0179,  0.0306, -0.0147], device='cuda:0'),\n",
       "  tensor([-3.2677,  6.1335, -3.2955], device='cuda:0'))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_decomps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ccea5db-d9ad-4468-8e36-3f0f0903dd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_decomps[11][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "882e2b06-770b-4e60-8031-76693614b5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0, 64])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_decomps[0][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41caeed-2bc6-4d62-9ff7-8b382ebfe062",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Function description:\n",
    "\n",
    "prop_classifier_model_hh_batched(encoding, model, source_list, target_nodes):\n",
    "\n",
    "- encoding - Encoding given by tokenizer\n",
    "- model - BERT model\n",
    "- source_list - List of lists where each list consists of tuples (layer, position, head) indexing a particular attention head whose influence is to be calculated\n",
    "- target_nodes - A single list of tuples (layer, position, head) containing attention heads on whom the influence is to be measured\n",
    "- num_at_time (optional) - Number of source_lists to be processed in a batch\n",
    "- n_layers - Number of layers\n",
    "- att_list - Attention probabilities if precomputed\n",
    "\n",
    "Output consists of two lists - out_decomps and target_decomps:\n",
    "- out_decomps - Consists of a list of tuples (rel, irrel) reflecting the decomposition of the _output_\n",
    "- target_decomps - A list containining 12 (one for each layer) where each list is of length len(source_list). For any layer l, each entry of target_decomps[l] is a tuple (rel, irrel) decomposition of the target nodes at that layer for the corresponding set of source nodes. rel, irrel are of dimension #number of target nodes in layer l x head_size and the ordering of the target nodes in this layer is the same as provided "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
