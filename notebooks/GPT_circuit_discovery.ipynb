{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be77102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'CD_Circuit/'\n",
      "/home/shawnghu/ml/CD_Circuit/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawnghu/ml/CD_Circuit/.venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "# This is for if we're trying to execute on a remote JupyterHub, where the pwd is set to the server root, or else I think pwd is set correctly already.\n",
    "%cd CD_Circuit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# CD-T Imports\n",
    "import math\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson, combine_token_attn, compute_word_intervals\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.cdt_basic import *\n",
    "from pyfunctions.cdt_source_to_target import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6d6ada4-4781-4789-b3b1-1d044c11b3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa2b131d4f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load Model\n",
    "\n",
    "Note: Unlike with the BERT model + medical dataset objective, it is not necessary to pretrain GPT-2 to perform the IOI dataset.\n",
    "GPT-2-small is already capable of performing IOI; that's part of the point of the Mech Interp in the Wild paper.\n",
    "We only need to examine how it does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3183be1-3bf6-4f5a-8134-9bdd83db0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a520f760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "# This makes some sense, since EasyTransformer, the repo/lib released by the IOI guys, was forked from TransformerLens.\n",
    "# In fact, this makes the reproduction a little bit more faithful, since they most likely do certain things such as \n",
    "# \"folding\" LayerNorms to improve their interpretability results, and we are able to do the same by using TransformerLens.\n",
    "# HuggingFace, by contrast, has the most impenetrable docs and tons of outdated APIs and etc.; even their source \n",
    "# code is impossible to traverse, and I gave up on it, thankfully quickly.\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28216f8c",
   "metadata": {},
   "source": [
    "## Example forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fb595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "tokens = model.to_tokens(text).to(device)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "probs = logits.softmax(dim=-1)\n",
    "most_likely_next_tokens = model.tokenizer.batch_decode(logits.argmax(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023f10b",
   "metadata": {},
   "source": [
    "## Inspect model or hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fa000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.blocks[0].attn.W_Q.shape)\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "print((encoding)) #effective attrs 'input_ids' and 'attention_mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1124eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "def compare_same(a, b, atol=1e-4, rtol=1e-3):\n",
    "    if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\n",
    "        comparison = torch.isclose(a, b, atol, rtol)\n",
    "        print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "        return\n",
    "    comparison = np.isclose(a, b, atol, rtol)\n",
    "    print(f\"{comparison.sum()/comparison.size:.2%} of the values are correct\")\n",
    "\n",
    "# confirm what some of these hook names mean-- namely, that hook_resid_post is the output of the entire transformer block\n",
    "compare_same(cache['blocks.0.hook_resid_post'], cache['blocks.1.hook_resid_pre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dbf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 17, 768)\n",
      "hook_pos_embed                 (1, 17, 768)\n",
      "blocks.0.hook_resid_pre        (1, 17, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 17, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 17, 768)\n",
      "blocks.0.attn.hook_q           (1, 17, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 17, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 17, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 17, 17)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 17, 17)\n",
      "blocks.0.attn.hook_z           (1, 17, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 17, 768)\n",
      "blocks.0.hook_resid_mid        (1, 17, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 17, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 17, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 17, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 17, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 17, 768)\n",
      "blocks.0.hook_resid_post       (1, 17, 768)\n",
      "ln_final.hook_scale            (1, 17, 1)\n",
      "ln_final.hook_normalized       (1, 17, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8823997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same as in the notebook, example\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \"Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377977-fe3b-45dd-9d00-1c19e5366038",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Prepare attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92798f42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nout_decomps, target_decomps = prop_model_hh_batched(encoding_idxs, attention_mask, model, source_list, target_nodes,\\n                                                                   device=device,\\n                                                                   patched_values=None, mean_ablated=False, num_at_time=1)\\n                                                                   # patched_values=mean_act, mean_ablated=True)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test\n",
    "pos_specific_hs = [\n",
    "        [i for i in range(12)],\n",
    "        [0],\n",
    "        [i for i in range(12)]\n",
    "    ]\n",
    "all_heads = list(itertools.product(*pos_specific_hs))\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)] # not meaningful in a GPT context\n",
    "source_list = [[node] for node in all_heads if node not in target_nodes]\n",
    "'''\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "encoding_idxs, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "input_shape = encoding_idxs.size()\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "'''\n",
    "out_decomps, target_decomps = prop_model_hh_batched(encoding_idxs, attention_mask, model, source_list, target_nodes,\n",
    "                                                                   device=device,\n",
    "                                                                   patched_values=None, mean_ablated=False, num_at_time=1)\n",
    "                                                                   # patched_values=mean_act, mean_ablated=True)\n",
    "'''                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations\n",
    "\n",
    "This is not as simple as it sounds; for the IOI paper, for each individual input following a template, they ablate using the mean activations of the \"ABC\" dataset, generated over sentences following the same template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7b521-61cb-41af-8ed9-48bdee9cd726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ab88048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "\n",
    "# Generate a dataset all consisting of one template, randomly chosen.\n",
    "# nb_templates = 2 due to some logic internal to IOIDataset:\n",
    "# essentially, the nouns can be an ABBA or ABAB order and that counts as separate templates.\n",
    "ioi_dataset = IOIDataset(prompt_type=\"mixed\", N=3, tokenizer=model.tokenizer, prepend_bos=False, nb_templates=2)\n",
    "\n",
    "# This is the P_ABC that is mentioned in the IOI paper, which we use for mean ablation.\n",
    "# Importantly, passing in prompt_type=\"ABC\" or similar is NOT the same thing as this.\n",
    "abc_dataset = (\n",
    "    ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b151924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_dataset.toks.shape\n",
    "ioi_dataset.toks[0:1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and mean_acts must be pickled together, since the mean activations are specific\n",
    "# to the template type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1df34b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(abc_dataset.toks) # run on entire dataset along batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab12e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 16, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs = [cache['blocks.' + str(i) + '.hook_attn_out'] for i in range(12)]\n",
    "attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, head, seq, d_model\n",
    "mean_acts = torch.mean(attention_outputs, dim=0)\n",
    "mean_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecddf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86c32de5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_list = [(0, 0, 0), (1, 1, 1)]\n",
    "target_nodes = [(7, 0, 1)]\n",
    "\n",
    "text = ioi_dataset.sentences[0]\n",
    "encoding = model.tokenizer.encode_plus(text, \n",
    "                                 add_special_tokens=True, \n",
    "                                 max_length=512,\n",
    "                                 truncation=True, \n",
    "                                 padding = \"longest\", \n",
    "                                 return_attention_mask=True, \n",
    "                                 return_tensors=\"pt\").to(device)\n",
    "encoding_idxs, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "input_shape = encoding_idxs.size()\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "print(input_shape)\n",
    "print(attention_mask)\n",
    "out_decomps, target_decomps, _ = prop_GPT(encoding_idxs, extended_attention_mask, model, source_list, target_nodes, mean_acts=mean_acts, set_irrel_to_mean=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4db7784d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(11, 0, 1): [0, 1, 2]}\n",
      "torch.Size([3, 16, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 16, 50257)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_decomps, target_decomps, _ = prop_GPT(ioi_dataset.toks[0:3, :], extended_attention_mask, model, source_list, target_nodes, mean_acts=mean_acts, set_irrel_to_mean=True, device=device)\n",
    "out_decomps[0].rel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964cc25",
   "metadata": {},
   "source": [
    "Note it is possible to do the analysis \"batchwise\" by just taking the mean of the relevance scores (at time of writing, this detail isn't in the paper, I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb4c809f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "ranges = [\n",
    "        [layer for layer in range(11, 12)],\n",
    "        [sequence_position for sequence_position in range(input_shape[-1])],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = list(itertools.product(*ranges))\n",
    "# print(source_nodes[:64])\n",
    "# target_nodes = [(7, 0, 1)]\n",
    "target_nodes = []\n",
    "\n",
    "# prop_fn = lambda snl: prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, snl, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "# out_decomps, target_decomps = batch_run(prop_fn, source_nodes)\n",
    "out_decomps = []\n",
    "# batching is broken for now, just run one by one\n",
    "for layer in range(11, 12):\n",
    "    for sequence_position in range(input_shape[-1]):\n",
    "        for attention_head_idx in range(12):\n",
    "            source_node = (layer, sequence_position, attention_head_idx)\n",
    "            target_nodes = []\n",
    "            out_decomp, _, _ = prop_GPT(encoding_idxs[0:1], extended_attention_mask, model, [source_node], target_nodes, mean_acts=mean_acts, set_irrel_to_mean=True, device=device)\n",
    "            out_decomps.append(out_decomp[0])            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3cbdd4d-c39e-49d0-8a25-1aadb7609e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6219, 45187, 3362]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ioi_dataset.io_tokenIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cad70e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each source node determine the contribution of rel to the actual score\n",
    "\n",
    "logits = (out_decomps[0].rel + out_decomps[0].irrel) # 1, seq_len, 50257=d_vocab\n",
    "io_logit = logits[0, -1, ioi_dataset.io_tokenIDs[0]]\n",
    "s_logit = logits[0, -1, ioi_dataset.s_tokenIDs[0]]\n",
    "full_score = io_logit - s_logit\n",
    "\n",
    "results = []\n",
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', ('source_node', 'score'))\n",
    "\n",
    "for decomp in out_decomps:\n",
    "    rel_io_logit = decomp.rel[0, -1, ioi_dataset.io_tokenIDs[0]]\n",
    "    rel_s_logit = decomp.rel[0, -1, ioi_dataset.s_tokenIDs[0]]\n",
    "    score = rel_io_logit - rel_s_logit\n",
    "    norm_score = score / full_score\n",
    "    results.append(Result(decomp.source_node, norm_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a98c112f-bd38-4eb7-9ba8-a8be0c3de18c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_same(a, b, atol=1e-4, rtol=1e-3):\n",
    "    if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\n",
    "        comparison = torch.isclose(a, b, atol, rtol)\n",
    "        print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "        return\n",
    "    comparison = np.isclose(a, b, atol, rtol)\n",
    "    print(f\"{comparison.sum()/comparison.size:.2%} of the values are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5e93435-8453-46c6-8abe-aa556e398ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "out_decomps[132].source_node\n",
    "compare_same(out_decomps[132].rel, out_decomps[133].rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb241388-4c18-4ee4-9368-f8b389fb01ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_list = [(11, 0, 0)]\n",
    "out_decomps, target_decomps, _ = prop_GPT(ioi_dataset.toks[0:1, :], extended_attention_mask, model, source_list, target_nodes, mean_acts=mean_acts, set_irrel_to_mean=True, device=device)\n",
    "rel1 = out_decomps[0].rel\n",
    "\n",
    "source_list = [(11, 0, 1)]\n",
    "out_decomps, target_decomps, _ = prop_GPT(ioi_dataset.toks[0:1, :], extended_attention_mask, model, source_list, target_nodes, mean_acts=mean_acts, set_irrel_to_mean=True, device=device)\n",
    "rel2 = out_decomps[0].rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fce2d8c6-cf62-4360-be87-ad0388797866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.98% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "compare_same(rel1, rel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a25972ca-181f-43bc-9750-acfb689a7687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "results.sort(key=operator.attrgetter('score'), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4401803-697d-4dc3-8212-fd99658257a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(source_node=(11, 15, 2), score=0.28352752),\n",
       " Result(source_node=(11, 15, 0), score=0.27941012),\n",
       " Result(source_node=(11, 15, 10), score=0.26351264),\n",
       " Result(source_node=(11, 15, 6), score=0.2276937),\n",
       " Result(source_node=(11, 1, 4), score=0.20821679),\n",
       " Result(source_node=(11, 15, 11), score=0.20689419),\n",
       " Result(source_node=(11, 3, 11), score=0.20282805),\n",
       " Result(source_node=(11, 3, 4), score=0.19099568),\n",
       " Result(source_node=(11, 1, 10), score=0.18760513),\n",
       " Result(source_node=(11, 1, 2), score=0.18759945),\n",
       " Result(source_node=(11, 1, 3), score=0.18734848),\n",
       " Result(source_node=(11, 1, 9), score=0.18715921),\n",
       " Result(source_node=(11, 1, 11), score=0.18461661),\n",
       " Result(source_node=(11, 1, 6), score=0.18450382),\n",
       " Result(source_node=(11, 1, 8), score=0.18422359),\n",
       " Result(source_node=(11, 3, 3), score=0.18390124),\n",
       " Result(source_node=(11, 3, 7), score=0.1837617),\n",
       " Result(source_node=(11, 1, 7), score=0.1815961),\n",
       " Result(source_node=(11, 15, 3), score=0.17958887),\n",
       " Result(source_node=(11, 1, 0), score=0.17948277),\n",
       " Result(source_node=(11, 9, 4), score=0.17869861),\n",
       " Result(source_node=(11, 1, 5), score=0.17803855),\n",
       " Result(source_node=(11, 3, 10), score=0.17759669),\n",
       " Result(source_node=(11, 0, 7), score=0.17630012),\n",
       " Result(source_node=(11, 1, 1), score=0.17603505),\n",
       " Result(source_node=(11, 3, 6), score=0.17548952),\n",
       " Result(source_node=(11, 0, 1), score=0.17547286),\n",
       " Result(source_node=(11, 0, 2), score=0.17456256),\n",
       " Result(source_node=(11, 3, 2), score=0.17019156),\n",
       " Result(source_node=(11, 0, 9), score=0.17001836),\n",
       " Result(source_node=(11, 0, 5), score=0.16803837),\n",
       " Result(source_node=(11, 0, 8), score=0.16770662),\n",
       " Result(source_node=(11, 0, 6), score=0.16769409),\n",
       " Result(source_node=(11, 8, 2), score=0.1675872),\n",
       " Result(source_node=(11, 0, 4), score=0.16741382),\n",
       " Result(source_node=(11, 0, 11), score=0.16636102),\n",
       " Result(source_node=(11, 0, 3), score=0.16572936),\n",
       " Result(source_node=(11, 2, 4), score=0.16548352),\n",
       " Result(source_node=(11, 0, 0), score=0.16511294),\n",
       " Result(source_node=(11, 15, 4), score=0.16446386),\n",
       " Result(source_node=(11, 4, 0), score=0.16419813),\n",
       " Result(source_node=(11, 0, 10), score=0.16404198),\n",
       " Result(source_node=(11, 8, 6), score=0.16303563),\n",
       " Result(source_node=(11, 2, 10), score=0.16232099),\n",
       " Result(source_node=(11, 2, 2), score=0.16225696),\n",
       " Result(source_node=(11, 8, 0), score=0.16170649),\n",
       " Result(source_node=(11, 4, 1), score=0.16139947),\n",
       " Result(source_node=(11, 4, 4), score=0.16105378),\n",
       " Result(source_node=(11, 4, 9), score=0.1609013),\n",
       " Result(source_node=(11, 4, 5), score=0.15998602),\n",
       " Result(source_node=(11, 4, 3), score=0.1598896),\n",
       " Result(source_node=(11, 4, 2), score=0.159204),\n",
       " Result(source_node=(11, 4, 11), score=0.15899973),\n",
       " Result(source_node=(11, 4, 10), score=0.15869665),\n",
       " Result(source_node=(11, 9, 3), score=0.1585274),\n",
       " Result(source_node=(11, 2, 7), score=0.15766321),\n",
       " Result(source_node=(11, 4, 8), score=0.15742338),\n",
       " Result(source_node=(11, 9, 9), score=0.15740453),\n",
       " Result(source_node=(11, 2, 5), score=0.15723635),\n",
       " Result(source_node=(11, 5, 10), score=0.15721475),\n",
       " Result(source_node=(11, 5, 1), score=0.15716456),\n",
       " Result(source_node=(11, 7, 4), score=0.15714009),\n",
       " Result(source_node=(11, 3, 5), score=0.15709843),\n",
       " Result(source_node=(11, 4, 6), score=0.15699436),\n",
       " Result(source_node=(11, 3, 1), score=0.15691291),\n",
       " Result(source_node=(11, 9, 0), score=0.15687574),\n",
       " Result(source_node=(11, 8, 3), score=0.15595682),\n",
       " Result(source_node=(11, 8, 4), score=0.15500554),\n",
       " Result(source_node=(11, 5, 11), score=0.1548522),\n",
       " Result(source_node=(11, 5, 3), score=0.15460414),\n",
       " Result(source_node=(11, 8, 10), score=0.15457994),\n",
       " Result(source_node=(11, 3, 0), score=0.15449673),\n",
       " Result(source_node=(11, 2, 1), score=0.15430595),\n",
       " Result(source_node=(11, 9, 11), score=0.15410855),\n",
       " Result(source_node=(11, 5, 6), score=0.15409294),\n",
       " Result(source_node=(11, 4, 7), score=0.15408559),\n",
       " Result(source_node=(11, 5, 2), score=0.15371403),\n",
       " Result(source_node=(11, 8, 5), score=0.15313266),\n",
       " Result(source_node=(11, 5, 7), score=0.15307933),\n",
       " Result(source_node=(11, 5, 5), score=0.15259634),\n",
       " Result(source_node=(11, 5, 4), score=0.15245621),\n",
       " Result(source_node=(11, 5, 0), score=0.15241957),\n",
       " Result(source_node=(11, 9, 7), score=0.15221444),\n",
       " Result(source_node=(11, 5, 9), score=0.15175048),\n",
       " Result(source_node=(11, 8, 1), score=0.15157099),\n",
       " Result(source_node=(11, 5, 8), score=0.15152317),\n",
       " Result(source_node=(11, 2, 6), score=0.15148975),\n",
       " Result(source_node=(11, 3, 8), score=0.1513919),\n",
       " Result(source_node=(11, 2, 0), score=0.150512),\n",
       " Result(source_node=(11, 2, 9), score=0.15023087),\n",
       " Result(source_node=(11, 8, 7), score=0.14982362),\n",
       " Result(source_node=(11, 2, 11), score=0.1492941),\n",
       " Result(source_node=(11, 7, 10), score=0.14917414),\n",
       " Result(source_node=(11, 6, 6), score=0.14902537),\n",
       " Result(source_node=(11, 8, 8), score=0.14887057),\n",
       " Result(source_node=(11, 8, 9), score=0.14857122),\n",
       " Result(source_node=(11, 9, 5), score=0.14790289),\n",
       " Result(source_node=(11, 3, 9), score=0.1477288),\n",
       " Result(source_node=(11, 7, 2), score=0.14752653),\n",
       " Result(source_node=(11, 9, 2), score=0.14676446),\n",
       " Result(source_node=(11, 9, 1), score=0.14655763),\n",
       " Result(source_node=(11, 6, 8), score=0.14643578),\n",
       " Result(source_node=(11, 6, 0), score=0.14641292),\n",
       " Result(source_node=(11, 6, 4), score=0.14632687),\n",
       " Result(source_node=(11, 6, 9), score=0.14562204),\n",
       " Result(source_node=(11, 2, 8), score=0.14539608),\n",
       " Result(source_node=(11, 2, 3), score=0.14533181),\n",
       " Result(source_node=(11, 6, 11), score=0.14530489),\n",
       " Result(source_node=(11, 6, 5), score=0.14523542),\n",
       " Result(source_node=(11, 7, 6), score=0.14479117),\n",
       " Result(source_node=(11, 6, 7), score=0.14475927),\n",
       " Result(source_node=(11, 9, 8), score=0.14453611),\n",
       " Result(source_node=(11, 6, 2), score=0.14431591),\n",
       " Result(source_node=(11, 7, 8), score=0.14393839),\n",
       " Result(source_node=(11, 6, 3), score=0.14388329),\n",
       " Result(source_node=(11, 7, 5), score=0.14350326),\n",
       " Result(source_node=(11, 7, 3), score=0.14330694),\n",
       " Result(source_node=(11, 7, 0), score=0.14270364),\n",
       " Result(source_node=(11, 9, 6), score=0.14270265),\n",
       " Result(source_node=(11, 6, 10), score=0.14229117),\n",
       " Result(source_node=(11, 7, 11), score=0.14159262),\n",
       " Result(source_node=(11, 6, 1), score=0.14135762),\n",
       " Result(source_node=(11, 10, 5), score=0.14132395),\n",
       " Result(source_node=(11, 7, 7), score=0.1411266),\n",
       " Result(source_node=(11, 7, 9), score=0.14055932),\n",
       " Result(source_node=(11, 10, 0), score=0.14018346),\n",
       " Result(source_node=(11, 7, 1), score=0.1386859),\n",
       " Result(source_node=(11, 8, 11), score=0.13836302),\n",
       " Result(source_node=(11, 9, 10), score=0.13747029),\n",
       " Result(source_node=(11, 10, 1), score=0.13255572),\n",
       " Result(source_node=(11, 10, 2), score=0.13058731),\n",
       " Result(source_node=(11, 10, 6), score=0.12932634),\n",
       " Result(source_node=(11, 10, 8), score=0.12621464),\n",
       " Result(source_node=(11, 0, 1), score=0.12618476),\n",
       " Result(source_node=(11, 10, 4), score=0.12168392),\n",
       " Result(source_node=(11, 10, 3), score=0.12099905),\n",
       " Result(source_node=(11, 10, 11), score=0.12076673),\n",
       " Result(source_node=(11, 10, 10), score=0.12034184),\n",
       " Result(source_node=(11, 10, 9), score=0.11765304),\n",
       " Result(source_node=(11, 10, 7), score=0.11670035),\n",
       " Result(source_node=(11, 15, 5), score=0.09712926),\n",
       " Result(source_node=(11, 15, 8), score=0.08904097),\n",
       " Result(source_node=(11, 14, 4), score=0.08437527),\n",
       " Result(source_node=(11, 15, 9), score=0.0827132),\n",
       " Result(source_node=(11, 12, 4), score=0.075276904),\n",
       " Result(source_node=(11, 15, 7), score=0.07094683),\n",
       " Result(source_node=(11, 12, 5), score=0.060980383),\n",
       " Result(source_node=(11, 15, 1), score=0.060835842),\n",
       " Result(source_node=(11, 12, 10), score=0.058796942),\n",
       " Result(source_node=(11, 12, 1), score=0.05791432),\n",
       " Result(source_node=(11, 14, 10), score=0.05697748),\n",
       " Result(source_node=(11, 12, 8), score=0.055993307),\n",
       " Result(source_node=(11, 13, 6), score=0.05584915),\n",
       " Result(source_node=(11, 12, 6), score=0.05549755),\n",
       " Result(source_node=(11, 12, 9), score=0.055390794),\n",
       " Result(source_node=(11, 12, 0), score=0.055094596),\n",
       " Result(source_node=(11, 12, 7), score=0.053610872),\n",
       " Result(source_node=(11, 12, 3), score=0.053506),\n",
       " Result(source_node=(11, 14, 6), score=0.053464185),\n",
       " Result(source_node=(11, 11, 6), score=0.05310436),\n",
       " Result(source_node=(11, 12, 2), score=0.052812003),\n",
       " Result(source_node=(11, 11, 1), score=0.05274778),\n",
       " Result(source_node=(11, 11, 11), score=0.050704382),\n",
       " Result(source_node=(11, 11, 2), score=0.05010727),\n",
       " Result(source_node=(11, 12, 11), score=0.05005367),\n",
       " Result(source_node=(11, 11, 5), score=0.049676884),\n",
       " Result(source_node=(11, 11, 0), score=0.04963051),\n",
       " Result(source_node=(11, 14, 2), score=0.048821926),\n",
       " Result(source_node=(11, 11, 10), score=0.048367277),\n",
       " Result(source_node=(11, 14, 8), score=0.048345536),\n",
       " Result(source_node=(11, 11, 3), score=0.0478527),\n",
       " Result(source_node=(11, 11, 4), score=0.047682293),\n",
       " Result(source_node=(11, 14, 1), score=0.04723617),\n",
       " Result(source_node=(11, 11, 9), score=0.04721033),\n",
       " Result(source_node=(11, 11, 7), score=0.04630628),\n",
       " Result(source_node=(11, 13, 4), score=0.045613233),\n",
       " Result(source_node=(11, 11, 8), score=0.04536263),\n",
       " Result(source_node=(11, 14, 11), score=0.044702686),\n",
       " Result(source_node=(11, 13, 1), score=0.044631854),\n",
       " Result(source_node=(11, 13, 2), score=0.0442314),\n",
       " Result(source_node=(11, 14, 5), score=0.044118967),\n",
       " Result(source_node=(11, 13, 9), score=0.043659914),\n",
       " Result(source_node=(11, 14, 9), score=0.04191031),\n",
       " Result(source_node=(11, 13, 8), score=0.04170542),\n",
       " Result(source_node=(11, 13, 10), score=0.04146527),\n",
       " Result(source_node=(11, 13, 3), score=0.041371766),\n",
       " Result(source_node=(11, 14, 3), score=0.040654834),\n",
       " Result(source_node=(11, 13, 0), score=0.040504552),\n",
       " Result(source_node=(11, 13, 7), score=0.039848924),\n",
       " Result(source_node=(11, 13, 11), score=0.039594024),\n",
       " Result(source_node=(11, 13, 5), score=0.038272787),\n",
       " Result(source_node=(11, 14, 0), score=0.037856266),\n",
       " Result(source_node=(11, 14, 7), score=0.036628753)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce463474-bb7b-42c5-84bc-8fac38699c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "scores = [result.score for result in results]\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.hist(scores, density=True, bins=192)  # density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc3f6b47-439f-4fe0-b987-a8b2a1264258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bisect\n",
    "def interquartile_range(results, iqr_multiplier=1):\n",
    "    # assume sorted in increasing order\n",
    "    third_quartile = results[int(len(results) * 0.75)].score\n",
    "    first_quartile = results[int(len(results) * 0.25)].score\n",
    "    IQR = third_quartile - first_quartile\n",
    "    outlier_score = third_quartile + iqr_multiplier * IQR\n",
    "    print(third_quartile, first_quartile, outlier_score)\n",
    "\n",
    "    by_score = operator.attrgetter('score')\n",
    "    outliers = results[bisect.bisect(results, outlier_score, key=by_score):]\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26ff1e97-c772-4063-8b4c-e9705185498e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n_sigma_rule(results, n=3):\n",
    "    scores = [r.score for r in results]\n",
    "    std_dev = np.std(scores)\n",
    "    mean = np.mean(scores)\n",
    "    outlier_score = mean + std_dev * n\n",
    "    print(std_dev, mean, outlier_score)\n",
    "    by_score = operator.attrgetter('score')\n",
    "    outliers = results[bisect.bisect(results, outlier_score, key=by_score):]\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3380ed9f-341b-4f32-9291-0e197498b65d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from '/usr/local/linux/miniforge-3.12/lib/python3.12/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "effd20fb-68cd-41df-99a8-a2c44bff8038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1609013 0.075276904 0.24652568250894547\n",
      "0.05273454 0.12999935 0.23546843230724335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(source_node=(11, 15, 10), score=0.26351264),\n",
       " Result(source_node=(11, 15, 0), score=0.27941012),\n",
       " Result(source_node=(11, 15, 2), score=0.28352752)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort(key=operator.attrgetter('score'), reverse=False)\n",
    "interquartile_range(results)\n",
    "n_sigma_rule(results, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d00b5-4527-48e9-b84d-1df046336b6f",
   "metadata": {},
   "source": [
    "# One more iteration: Find maximally relevant source nodes to target nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2b05c-eee7-4a95-aef5-126b082ec550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
