{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# CD-T Imports\n",
    "import math\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson, combine_token_attn, compute_word_intervals\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.patch_hh import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6ada4-4781-4789-b3b1-1d044c11b3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7e816c9c9670>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3183be1-3bf6-4f5a-8134-9bdd83db0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a520f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "# This makes some sense, since EasyTransformer, the repo/lib released by the IOI guys, was forked from TransformerLens.\n",
    "# In fact, this makes the reproduction a little bit more faithful, since they most likely do certain things such as \n",
    "# \"folding\" LayerNorms to improve their interpretability results, and we are able to do the same by using TransformerLens.\n",
    "# HuggingFace, by contrast, has the most impenetrable docs and tons of outdated APIs and etc.; even their source \n",
    "# code is impossible to traverse, and I gave up on it, thankfully quickly.\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8f76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg.default_prepend_bos = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3c253",
   "metadata": {},
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529e2aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': False,\n",
       " 'device': device(type='cpu'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LN',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.default_prepend_bos # prepends a beginning-of-sequence token? but GPT2 doesn't do this, I think?\n",
    "model.tokenizer.padding_side # 'right'\n",
    "model.cfg.positional_embedding_type # 'standard', as opposed to 'shortformer', which puts the positional embedding in the attention pattern\n",
    "model.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28216f8c",
   "metadata": {},
   "source": [
    "## Example forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fb595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "tokens = model.to_tokens(text, padding_side='right', truncate=False).to(device)\n",
    "print(tokens.shape)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "probs = logits.softmax(dim=-1)\n",
    "most_likely_next_tokens = model.tokenizer.batch_decode(logits.argmax(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023f10b",
   "metadata": {},
   "source": [
    "## Inspect model / hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d299611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OV',\n",
       " 'QK',\n",
       " 'T_destination',\n",
       " 'W_E',\n",
       " 'W_E_pos',\n",
       " 'W_K',\n",
       " 'W_O',\n",
       " 'W_Q',\n",
       " 'W_U',\n",
       " 'W_V',\n",
       " 'W_gate',\n",
       " 'W_in',\n",
       " 'W_out',\n",
       " 'W_pos',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_enable_hook',\n",
       " '_enable_hook_with_name',\n",
       " '_enable_hooks_for_points',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_init_weights_gpt2',\n",
       " '_init_weights_kaiming',\n",
       " '_init_weights_muP',\n",
       " '_init_weights_xavier',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'accumulated_bias',\n",
       " 'add_caching_hooks',\n",
       " 'add_hook',\n",
       " 'add_module',\n",
       " 'add_perma_hook',\n",
       " 'all_composition_scores',\n",
       " 'all_head_labels',\n",
       " 'apply',\n",
       " 'b_K',\n",
       " 'b_O',\n",
       " 'b_Q',\n",
       " 'b_U',\n",
       " 'b_V',\n",
       " 'b_in',\n",
       " 'b_out',\n",
       " 'bfloat16',\n",
       " 'blocks',\n",
       " 'buffers',\n",
       " 'cache_all',\n",
       " 'cache_some',\n",
       " 'call_super_init',\n",
       " 'center_unembed',\n",
       " 'center_writing_weights',\n",
       " 'cfg',\n",
       " 'check_and_add_hook',\n",
       " 'check_hooks_to_add',\n",
       " 'children',\n",
       " 'clear_contexts',\n",
       " 'compile',\n",
       " 'context_level',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'dataset',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fill_missing_keys',\n",
       " 'float',\n",
       " 'fold_layer_norm',\n",
       " 'fold_value_biases',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'from_pretrained_no_processing',\n",
       " 'generate',\n",
       " 'get_buffer',\n",
       " 'get_caching_hooks',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'get_token_position',\n",
       " 'half',\n",
       " 'hook_dict',\n",
       " 'hook_embed',\n",
       " 'hook_points',\n",
       " 'hook_pos_embed',\n",
       " 'hooks',\n",
       " 'init_weights',\n",
       " 'input_to_embed',\n",
       " 'ipu',\n",
       " 'is_caching',\n",
       " 'ln_final',\n",
       " 'load_and_process_state_dict',\n",
       " 'load_sample_training_dataset',\n",
       " 'load_state_dict',\n",
       " 'loss_fn',\n",
       " 'mod_dict',\n",
       " 'modules',\n",
       " 'move_model_modules_to_device',\n",
       " 'mps',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'pos_embed',\n",
       " 'process_weights_',\n",
       " 'refactor_factored_attn_matrices',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'remove_all_hook_fns',\n",
       " 'requires_grad_',\n",
       " 'reset_hooks',\n",
       " 'run_with_cache',\n",
       " 'run_with_hooks',\n",
       " 'sample_datapoint',\n",
       " 'set_extra_state',\n",
       " 'set_tokenizer',\n",
       " 'set_use_attn_in',\n",
       " 'set_use_attn_result',\n",
       " 'set_use_hook_mlp_in',\n",
       " 'set_use_split_qkv_input',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'to_single_str_token',\n",
       " 'to_single_token',\n",
       " 'to_str_tokens',\n",
       " 'to_string',\n",
       " 'to_tokens',\n",
       " 'tokenizer',\n",
       " 'tokens_to_residual_directions',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'unembed',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "430fa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'apply_mlp', 'attn', 'bfloat16', 'buffers', 'call_super_init', 'cfg', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'hook_attn_in', 'hook_attn_out', 'hook_k_input', 'hook_mlp_in', 'hook_mlp_out', 'hook_q_input', 'hook_resid_mid', 'hook_resid_post', 'hook_resid_pre', 'hook_v_input', 'ipu', 'ln1', 'ln2', 'load_state_dict', 'mlp', 'modules', 'name', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'normalization_type', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model.blocks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11dbf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 16, 768)\n",
      "hook_pos_embed                 (1, 16, 768)\n",
      "blocks.0.hook_resid_pre        (1, 16, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 16, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 16, 768)\n",
      "blocks.0.attn.hook_q           (1, 16, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 16, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 16, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 16, 16)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 16, 16)\n",
      "blocks.0.attn.hook_z           (1, 16, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 16, 768)\n",
      "blocks.0.hook_resid_mid        (1, 16, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 16, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 16, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 16, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 16, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 16, 768)\n",
      "blocks.0.hook_resid_post       (1, 16, 768)\n",
      "ln_final.hook_scale            (1, 16, 1)\n",
      "ln_final.hook_normalized       (1, 16, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377977-fe3b-45dd-9d00-1c19e5366038",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Test functioning of prop_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e67107",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prop_attention_hh() missing 3 required positional arguments: 'layer_patched_values', 'a_module', and 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter John and Mary went to the store, John gave a bottle of milk to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m encoding \u001b[38;5;241m=\u001b[39m get_encoding(text, model\u001b[38;5;241m.\u001b[39mtokenizer, device)\n\u001b[0;32m----> 6\u001b[0m out_decomps, target_decomps, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprop_GPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatched_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_ablated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/CD_Circuit/pyfunctions/patch_hh.py:300\u001b[0m, in \u001b[0;36mprop_GPT\u001b[0;34m(encoding, model, source_node_list, target_nodes, device, patched_values, att_list, output_att_prob, mean_ablated)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     layer_patched_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m rel, irrel, layer_target_decomps, returned_att_probs \u001b[38;5;241m=\u001b[39m \u001b[43mprop_GPT_layer_hh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mirrel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_node_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mlayer_patched_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mlayer_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43matt_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_att_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mmean_ablated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_ablated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m target_decomps\u001b[38;5;241m.\u001b[39mappend(layer_target_decomps)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# normalize_rel_irrel(rel_n, irrel_n)\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# rel, irrel = rel_n, irrel_n\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/CD_Circuit/pyfunctions/patch_hh.py:177\u001b[0m, in \u001b[0;36mprop_GPT_layer_hh\u001b[0;34m(rel, irrel, attention_mask, head_mask, source_node_list, target_nodes, level, layer_patched_values, layer_module, device, att_probs, output_att_prob, mean_ablated)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprop_GPT_layer_hh\u001b[39m(rel, irrel, attention_mask, head_mask, \n\u001b[1;32m    171\u001b[0m                   source_node_list, target_nodes, level, layer_patched_values,\n\u001b[1;32m    172\u001b[0m                   layer_module, device, att_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_att_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mean_ablated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# TODO: there should be some kind of casework for the folded layernorm,\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# if we want to perfectly apples-to-apples reproduce the IOI paper. \u001b[39;00m\n\u001b[1;32m    175\u001b[0m     rel_ln, irrel_ln \u001b[38;5;241m=\u001b[39m prop_layer_norm(rel, irrel, GPTLayerNormWrapper(layer_module\u001b[38;5;241m.\u001b[39mln1))\n\u001b[0;32m--> 177\u001b[0m     rel_attn_residual, irrel_attn_residual, returned_att_probs \u001b[38;5;241m=\u001b[39m \u001b[43mprop_attention_hh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_ln\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mirrel_ln\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43mGPTAttentionWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43matt_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_att_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     rel_mid, irrel_mid \u001b[38;5;241m=\u001b[39m prop_layer_norm(rel \u001b[38;5;241m+\u001b[39m rel_attn_residual, irrel \u001b[38;5;241m+\u001b[39m irrel_attn_residual, GPTLayerNormWrapper(layer_module\u001b[38;5;241m.\u001b[39mln2))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# MLP\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: prop_attention_hh() missing 3 required positional arguments: 'layer_patched_values', 'a_module', and 'device'"
     ]
    }
   ],
   "source": [
    "source_list = [(0, 0, 0), (1, 1, 1)]\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "out_decomps, target_decomps, _ = prop_GPT(encoding, model, source_list, target_nodes, device=device, patched_values=None, mean_ablated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d09d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(512, 50257)\n",
      "[[ 5.5012274  8.864304   3.4959507 ... -4.0528517 -2.6054635  7.205061 ]\n",
      " [ 5.5009346  8.865846   3.4954956 ... -4.053485  -2.6107361  7.206545 ]\n",
      " [ 5.499007   8.864225   3.4940565 ... -4.0518312 -2.6099834  7.2044487]\n",
      " ...\n",
      " [ 5.4935718  8.862474   3.4887767 ... -4.0536976 -2.6203256  7.198222 ]\n",
      " [ 5.4935718  8.862474   3.4887767 ... -4.0536976 -2.6203256  7.198222 ]\n",
      " [ 5.4935718  8.862474   3.4887767 ... -4.0536976 -2.6203256  7.198222 ]]\n"
     ]
    }
   ],
   "source": [
    "print(len(out_decomps)) # one per source node\n",
    "print(out_decomps[0].rel) \n",
    "print(out_decomps[0].rel.shape) # 512, d_vocab-- the effective output is this[-1], but most of this is zeroes, and is only nonzero at target node indices\n",
    "print(out_decomps[0].irrel)\n",
    "# TODO: given the formula we have calculated for relevance, is there a more compact representation of the output available?\n",
    "\n",
    "# TODO: calculate the relevance to logits using this function, or isn't it already calculated?\n",
    "# TODO: check correctness of various basic operations: if no target node specified, should irrel be the same as output? should sum over all input source nodes of output decomposition be equal to original output?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92798f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"NoneType\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m get_encoding(text, model\u001b[38;5;241m.\u001b[39mtokenizer, device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# encoding.input_ids.shape # 512-long vector, not sure why the tokens change from EOS to 0 at some point\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# embedding = model.embed(encoding.input_ids)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m out_decomps, target_decomps \u001b[38;5;241m=\u001b[39m \u001b[43mprop_model_hh_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mpatched_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_ablated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_at_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m                                                                    \u001b[38;5;66;03m# patched_values=mean_act, mean_ablated=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/CD_Circuit/pyfunctions/patch_hh.py:444\u001b[0m, in \u001b[0;36mprop_model_hh_batched\u001b[0;34m(encoding, model, source_node_list, target_nodes, device, patched_values, num_at_time, n_layers, att_list, output_att_prob, mean_ablated)\u001b[0m\n\u001b[1;32m    435\u001b[0m         layer_out_decomps, layer_target_decomps, att_probs_lst \u001b[38;5;241m=\u001b[39m prop_GPT(encoding, model, \n\u001b[1;32m    436\u001b[0m                                                                         source_node_list[b_st: b_end],\n\u001b[1;32m    437\u001b[0m                                                                         target_nodes, device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m                                                                         output_att_prob\u001b[38;5;241m=\u001b[39moutput_att_prob,\n\u001b[1;32m    441\u001b[0m                                                                         mean_ablated\u001b[38;5;241m=\u001b[39mmean_ablated)\n\u001b[1;32m    443\u001b[0m     out_decomps \u001b[38;5;241m=\u001b[39m out_decomps \u001b[38;5;241m+\u001b[39m layer_out_decomps\n\u001b[0;32m--> 444\u001b[0m     target_decomps \u001b[38;5;241m=\u001b[39m [\u001b[43mtarget_decomps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer_target_decomps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers)]\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_decomps, target_decomps\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"NoneType\") to list"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pos_specific_hs = [\n",
    "        [i for i in range(12)],\n",
    "        [0],\n",
    "        [i for i in range(12)]\n",
    "    ]\n",
    "all_heads = list(itertools.product(*pos_specific_hs))\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)] # not meaningful in a GPT context\n",
    "source_list = [[node] for node in all_heads if node not in target_nodes]\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "# encoding.input_ids.shape # 512-long vector, not sure why the tokens change from EOS to 0 at some point\n",
    "# embedding = model.embed(encoding.input_ids)\n",
    "\n",
    "out_decomps, target_decomps = prop_model_hh_batched(encoding, model, source_list, target_nodes,\n",
    "                                                                   device=device,\n",
    "                                                                   patched_values=None, mean_ablated=False, num_at_time=1)\n",
    "                                                                   # patched_values=mean_act, mean_ablated=True)\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cbe24",
   "metadata": {},
   "source": [
    "## Test correctness of forward pass\n",
    "\n",
    "Given that the above code runs, the forward pass \"type checks\", e.g, it's at least able to run. We still have to test correctness; this is especially hard given that TL GPT has so many discrepancies from HF BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb32d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_same(a, b, atol=1e-4, rtol=1e-3):\n",
    "    if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\n",
    "        comparison = torch.isclose(a, b, atol, rtol)\n",
    "        print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "        return\n",
    "    comparison = np.isclose(a, b, atol, rtol)\n",
    "    print(f\"{comparison.sum()/comparison.size:.2%} of the values are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "855602d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "# This loosely verifies that the out-decomposition's rel + irrel components form the output of the forward pass\n",
    "compare_same(out_decomps[0].rel + out_decomps[0].irrel, out_decomps[1].rel + out_decomps[1].irrel)\n",
    "\n",
    "# compare_same(cache['blocks.0.attn.hook_q'], cache['blocks.0.attn.hook_k']) # checking that there aren't the inputs, because the source code names them as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a376f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of the values are correct\n",
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "# compare_same(cache['hook_embed'], cache['blocks.0.hook_resid_pre'])\n",
    "# This just verifies an assumption about the input conventions to the model.\n",
    "in_0 = cache['blocks.0.hook_resid_pre']\n",
    "compare_same(cache['hook_embed'] + cache['hook_pos_embed'], cache['blocks.0.hook_resid_pre'])\n",
    "\n",
    "# Ensure that the encodings are the same (they won't be if you set prepend_bos=True for one and not the other!)\n",
    "# print(tokens)\n",
    "# print(encoding.input_ids)\n",
    "\n",
    "# Note that model.embed alone isn't the same thing as what we might normally call \"the embedding\"!\n",
    "embedding_output = model.embed(encoding.input_ids) + model.pos_embed(encoding.input_ids)\n",
    "compare_same(in_0, embedding_output[:, :16, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f2c3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n",
      "torch.Size([12, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "# Extra analysis of value matrix calcuation\n",
    "print(model.blocks[0].attn.W_V.shape)\n",
    "print(model.blocks[0].attn.W_O.shape)\n",
    "# print(model.blocks[0].attn.hook_pattern) # 1, 12, seq_len, seq_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da0edc71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m desired_attention_pattern \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.0.attn.hook_pattern\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# level = 0\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# source_node_list = [(0, 0, 0), (1, 1, 1)]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m rel_attn_residual, irrel_attn_residual, target_decomps, returned_att_probs \u001b[38;5;241m=\u001b[39m prop_self_attention_hh(rel_ln, irrel_ln, attention_mask, \n\u001b[1;32m     51\u001b[0m                                                                         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m                                                                         GPTAttentionWrapper(attention_module),\n\u001b[1;32m     53\u001b[0m                                                                         att_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_att_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m output \u001b[38;5;241m=\u001b[39m rel_attn_residual \u001b[38;5;241m+\u001b[39m irrel_attn_residual\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# output = rel_attn_residual + irrel_attn_residual + rel_ln + irrel_ln\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(\"Attention scores:\")\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(desired_attention_pattern.shape)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# print(returned_att_probs.shape)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# compare_same(returned_att_probs[0, :, :16, :16], desired_attention_pattern) # these do seem to be off by about 1e-4\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "# Test equivalence of attention layer\n",
    "from fancy_einsum import einsum\n",
    "'''\n",
    "blocks.0.attn.hook_q           (1, 17, 12, 64)\n",
    "blocks.0.attn.hook_k           (1, 17, 12, 64)\n",
    "blocks.0.attn.hook_v           (1, 17, 12, 64)\n",
    "blocks.0.attn.hook_attn_scores (1, 12, 17, 17)\n",
    "blocks.0.attn.hook_pattern     (1, 12, 17, 17)\n",
    "blocks.0.attn.hook_z           (1, 17, 12, 64)\n",
    "'''\n",
    "\n",
    "input_shape = encoding['input_ids'].size()\n",
    "\n",
    "attention_mask = get_extended_attention_mask(encoding['attention_mask'], \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "sh = list(embedding_output.shape)\n",
    "rel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel[:] = embedding_output[:]\n",
    "layer_module = model.blocks[0]\n",
    "attention_module = layer_module.attn\n",
    "\n",
    "'''\n",
    "rel_ln, irrel_ln = prop_layer_norm(rel, irrel, GPTLayerNormWrapper(layer_module.ln1))\n",
    "print(\"LayerNorm: \")\n",
    "compare_same(cache['blocks.0.ln1.hook_normalized'], (rel_ln + irrel_ln)[0, :16, :])\n",
    "\n",
    "desired_value_output = cache['blocks.0.attn.hook_v']\n",
    "rel_value, irrel_value = prop_linear(rel_ln, irrel_ln, GPTAttentionWrapper(attention_module, layer_module.ln2).value)\n",
    "value = rel_value + irrel_value\n",
    "print(\"Value matrix:\")\n",
    "# desired_value_output = desired_value_output.transpose(-1, -2)\n",
    "old_shape = desired_value_output.size()\n",
    "new_shape = old_shape[:-2] + (old_shape[-2] * old_shape[-1],)\n",
    "desired_value_output = desired_value_output.reshape(new_shape)\n",
    "print(desired_value_output.shape)\n",
    "print(value.shape)\n",
    "\n",
    "compare_same(value[0, :16, :], desired_value_output, rtol = 1e-2) # some elements differ by more than 1e-3 in proportion, not great\n",
    "'''\n",
    "\n",
    "# desired_block_output = cache['blocks.0.hook_resid_mid']\n",
    "desired_block_output = cache['blocks.0.hook_attn_out']\n",
    "desired_attention_pattern = cache['blocks.0.attn.hook_pattern']\n",
    "# level = 0\n",
    "# source_node_list = [(0, 0, 0), (1, 1, 1)]\n",
    "# target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "attn_wrapper = GPTAttentionWrapper(attention_module)\n",
    "rel_summed_values, irrel_summed_values, returned_att_probs = prop_self_attention_hh(rel_ln, irrel_ln, attention_mask, \n",
    "                                                                        None,\n",
    "                                                                        attn_wrapper,\n",
    "                                                                        att_probs=None, output_att_prob=True)\n",
    "rel_attn_residual, irrel_attn_residual = prop_linear(rel_summed_values, irrel_summed_values, attn_wrapper.output)\n",
    "\n",
    "output = rel_attn_residual + irrel_attn_residual\n",
    "# output = rel_attn_residual + irrel_attn_residual + rel_ln + irrel_ln\n",
    "\n",
    "# print(\"Attention scores:\")\n",
    "# print(desired_attention_pattern.shape)\n",
    "# print(returned_att_probs.shape)\n",
    "# compare_same(returned_att_probs[0, :, :16, :16], desired_attention_pattern) # these do seem to be off by about 1e-4\n",
    "\n",
    "print(\"Full attention block output: \")\n",
    "# old_shape = desired_block_output.size()\n",
    "# new_shape = old_shape[:-2] + (old_shape[-2] * old_shape[-1],)\n",
    "# desired_attention_output = desired_block_output.reshape(new_shape)\n",
    "print(desired_block_output.shape)\n",
    "print(output.shape)\n",
    "\n",
    "print(desired_block_output.sum())\n",
    "print(output[:, :16, :].sum())\n",
    "\n",
    "print(desired_block_output[0, :4, :4])\n",
    "print(output[0, :4, :4])\n",
    "compare_same(output[:, :16, :], desired_block_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab56041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test equivalence of entire transformer block\n",
    "in_1 = cache['blocks.1.hook_resid_pre']\n",
    "input_shape = encoding['input_ids'].size()\n",
    "\n",
    "extended_attention_mask = get_extended_attention_mask(encoding['attention_mask'], \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "sh = list(embedding_output.shape)\n",
    "rel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel[:] = embedding_output[:]\n",
    "layer_module = model.blocks[0]\n",
    "\n",
    "source_node_list = [(0, 0, 0), (1, 1, 1)]\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "rel_out, irrel_out, layer_target_decomps, _ = prop_GPT_layer_hh(rel, irrel, extended_attention_mask, \n",
    "                                                                                 None, source_node_list, \n",
    "                                                                                 target_nodes, 0, \n",
    "                                                                                 None,\n",
    "                                                                                 layer_module, \n",
    "                                                                                 device,\n",
    "                                                                                 None, False,\n",
    "                                                                                 mean_ablated=False)\n",
    "\n",
    "layer0_output = rel_out + irrel_out\n",
    "compare_same(in_1, layer0_output[:, :16, :])\n",
    "\n",
    "# want to check: layer_target_decomps is what we think it is (currently gated by, i didn't implement the decomposition part)\n",
    "# also, rel + irrel sums to the correct output\n",
    "layer_target_decomps[0].rel + layer_target_decomps[0].irrel == layer_target_decomps[1].rel + layer_target_decomps[1].irrel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43bc3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
