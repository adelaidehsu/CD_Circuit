{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# CD-T Imports\n",
    "import math\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson, combine_token_attn, compute_word_intervals\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.patch_hh import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6ada4-4781-4789-b3b1-1d044c11b3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7e56804eac60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3183be1-3bf6-4f5a-8134-9bdd83db0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a520f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "# This makes some sense, since EasyTransformer, the repo/lib released by the IOI guys, was forked from TransformerLens.\n",
    "# In fact, this makes the reproduction a little bit more faithful, since they most likely do certain things such as \n",
    "# \"folding\" LayerNorms to improve their interpretability results, and we are able to do the same by using TransformerLens.\n",
    "# HuggingFace, by contrast, has the most impenetrable docs and tons of outdated APIs and etc.; even their source \n",
    "# code is impossible to traverse, and I gave up on it, thankfully quickly.\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28216f8c",
   "metadata": {},
   "source": [
    "## Example forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fb595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "tokens = model.to_tokens(text).to(device)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "probs = logits.softmax(dim=-1)\n",
    "most_likely_next_tokens = model.tokenizer.batch_decode(logits.argmax(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023f10b",
   "metadata": {},
   "source": [
    "## Inspect model or hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430fa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n"
     ]
    }
   ],
   "source": [
    "print(model.blocks[0].attn.W_Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11dbf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 17, 768)\n",
      "hook_pos_embed                 (1, 17, 768)\n",
      "blocks.0.hook_resid_pre        (1, 17, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 17, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 17, 768)\n",
      "blocks.0.attn.hook_q           (1, 17, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 17, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 17, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 17, 17)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 17, 17)\n",
      "blocks.0.attn.hook_z           (1, 17, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 17, 768)\n",
      "blocks.0.hook_resid_mid        (1, 17, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 17, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 17, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 17, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 17, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 17, 768)\n",
      "blocks.0.hook_resid_post       (1, 17, 768)\n",
      "ln_final.hook_scale            (1, 17, 1)\n",
      "ln_final.hook_normalized       (1, 17, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377977-fe3b-45dd-9d00-1c19e5366038",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Test, currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = [(0, 0, 0), (1, 1, 1)]\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "out_decomps, target_decomps, _ = prop_GPT(encoding, model, source_list, target_nodes, device=device, patched_values=None, mean_ablated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d09d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(512, 50257)\n"
     ]
    }
   ],
   "source": [
    "print(len(out_decomps)) # one per source node\n",
    "print(out_decomps[0].rel) \n",
    "print(out_decomps[0].rel.shape) # 512, d_vocab-- the effective output is this[-1], but most of this is zeroes, and is only nonzero at target node indices\n",
    "# TODO: given the formula we have calculated for relevance, is there a more compact representation of the output available?\n",
    "\n",
    "# TODO: calculate the relevance to logits using this function, or isn't it already calculated?\n",
    "# TODO: check correctness of various basic operations: if no target node specified, should irrel be the same as output? should sum over all input source nodes of output decomposition be equal to original output?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92798f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "pos_specific_hs = [\n",
    "        [i for i in range(12)],\n",
    "        [0],\n",
    "        [i for i in range(12)]\n",
    "    ]\n",
    "all_heads = list(itertools.product(*pos_specific_hs))\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)] # not meaningful in a GPT context\n",
    "source_list = [[node] for node in all_heads if node not in target_nodes]\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "# encoding.input_ids.shape # 512-long vector, not sure why the tokens change from EOS to 0 at some point\n",
    "# embedding = model.embed(encoding.input_ids)\n",
    "\n",
    "out_decomps, target_decomps = prop_model_hh_batched(encoding, model, source_list, target_nodes,\n",
    "                                                                   device=device,\n",
    "                                                                   patched_values=None, mean_ablated=False, num_at_time=1)\n",
    "                                                                   # patched_values=mean_act, mean_ablated=True)\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43bc3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
