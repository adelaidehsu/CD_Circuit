{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# CD-T Imports\n",
    "import math\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson, combine_token_attn, compute_word_intervals\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.cdt_basic import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from pyfunctions.wrappers import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6ada4-4781-4789-b3b1-1d044c11b3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x71046f237800>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3183be1-3bf6-4f5a-8134-9bdd83db0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a520f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "# This makes some sense, since EasyTransformer, the repo/lib released by the IOI guys, was forked from TransformerLens.\n",
    "# In fact, this makes the reproduction a little bit more faithful, since they most likely do certain things such as \n",
    "# \"folding\" LayerNorms to improve their interpretability results, and we are able to do the same by using TransformerLens.\n",
    "# HuggingFace, by contrast, has the most impenetrable docs and tons of outdated APIs and etc.; even their source \n",
    "# code is impossible to traverse, and I gave up on it, thankfully quickly.\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8f76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg.default_prepend_bos = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3c253",
   "metadata": {},
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529e2aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': False,\n",
       " 'device': device(type='cpu'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LN',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.default_prepend_bos # prepends a beginning-of-sequence token? but GPT2 doesn't do this, I think?\n",
    "model.tokenizer.padding_side # 'right'\n",
    "model.cfg.positional_embedding_type # 'standard', as opposed to 'shortformer', which puts the positional embedding in the attention pattern\n",
    "model.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28216f8c",
   "metadata": {},
   "source": [
    "## Example forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fb595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "tokens = model.to_tokens(text, padding_side='right', truncate=False).to(device)\n",
    "print(tokens.shape)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "probs = logits.softmax(dim=-1)\n",
    "most_likely_next_tokens = model.tokenizer.batch_decode(logits.argmax(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023f10b",
   "metadata": {},
   "source": [
    "## Inspect model / hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d299611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OV',\n",
       " 'QK',\n",
       " 'T_destination',\n",
       " 'W_E',\n",
       " 'W_E_pos',\n",
       " 'W_K',\n",
       " 'W_O',\n",
       " 'W_Q',\n",
       " 'W_U',\n",
       " 'W_V',\n",
       " 'W_gate',\n",
       " 'W_in',\n",
       " 'W_out',\n",
       " 'W_pos',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_enable_hook',\n",
       " '_enable_hook_with_name',\n",
       " '_enable_hooks_for_points',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_init_weights_gpt2',\n",
       " '_init_weights_kaiming',\n",
       " '_init_weights_muP',\n",
       " '_init_weights_xavier',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'accumulated_bias',\n",
       " 'add_caching_hooks',\n",
       " 'add_hook',\n",
       " 'add_module',\n",
       " 'add_perma_hook',\n",
       " 'all_composition_scores',\n",
       " 'all_head_labels',\n",
       " 'apply',\n",
       " 'b_K',\n",
       " 'b_O',\n",
       " 'b_Q',\n",
       " 'b_U',\n",
       " 'b_V',\n",
       " 'b_in',\n",
       " 'b_out',\n",
       " 'bfloat16',\n",
       " 'blocks',\n",
       " 'buffers',\n",
       " 'cache_all',\n",
       " 'cache_some',\n",
       " 'call_super_init',\n",
       " 'center_unembed',\n",
       " 'center_writing_weights',\n",
       " 'cfg',\n",
       " 'check_and_add_hook',\n",
       " 'check_hooks_to_add',\n",
       " 'children',\n",
       " 'clear_contexts',\n",
       " 'compile',\n",
       " 'context_level',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'dataset',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fill_missing_keys',\n",
       " 'float',\n",
       " 'fold_layer_norm',\n",
       " 'fold_value_biases',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'from_pretrained_no_processing',\n",
       " 'generate',\n",
       " 'get_buffer',\n",
       " 'get_caching_hooks',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'get_token_position',\n",
       " 'half',\n",
       " 'hook_dict',\n",
       " 'hook_embed',\n",
       " 'hook_points',\n",
       " 'hook_pos_embed',\n",
       " 'hooks',\n",
       " 'init_weights',\n",
       " 'input_to_embed',\n",
       " 'ipu',\n",
       " 'is_caching',\n",
       " 'ln_final',\n",
       " 'load_and_process_state_dict',\n",
       " 'load_sample_training_dataset',\n",
       " 'load_state_dict',\n",
       " 'loss_fn',\n",
       " 'mod_dict',\n",
       " 'modules',\n",
       " 'move_model_modules_to_device',\n",
       " 'mps',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'pos_embed',\n",
       " 'process_weights_',\n",
       " 'refactor_factored_attn_matrices',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'remove_all_hook_fns',\n",
       " 'requires_grad_',\n",
       " 'reset_hooks',\n",
       " 'run_with_cache',\n",
       " 'run_with_hooks',\n",
       " 'sample_datapoint',\n",
       " 'set_extra_state',\n",
       " 'set_tokenizer',\n",
       " 'set_use_attn_in',\n",
       " 'set_use_attn_result',\n",
       " 'set_use_hook_mlp_in',\n",
       " 'set_use_split_qkv_input',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'to_single_str_token',\n",
       " 'to_single_token',\n",
       " 'to_str_tokens',\n",
       " 'to_string',\n",
       " 'to_tokens',\n",
       " 'tokenizer',\n",
       " 'tokens_to_residual_directions',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'unembed',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "430fa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'apply_mlp', 'attn', 'bfloat16', 'buffers', 'call_super_init', 'cfg', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'hook_attn_in', 'hook_attn_out', 'hook_k_input', 'hook_mlp_in', 'hook_mlp_out', 'hook_q_input', 'hook_resid_mid', 'hook_resid_post', 'hook_resid_pre', 'hook_v_input', 'ipu', 'ln1', 'ln2', 'load_state_dict', 'mlp', 'modules', 'name', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'normalization_type', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model.blocks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11dbf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 16, 768)\n",
      "hook_pos_embed                 (1, 16, 768)\n",
      "blocks.0.hook_resid_pre        (1, 16, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 16, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 16, 768)\n",
      "blocks.0.attn.hook_q           (1, 16, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 16, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 16, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 16, 16)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 16, 16)\n",
      "blocks.0.attn.hook_z           (1, 16, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 16, 768)\n",
      "blocks.0.hook_resid_mid        (1, 16, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 16, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 16, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 16, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 16, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 16, 768)\n",
      "blocks.0.hook_resid_post       (1, 16, 768)\n",
      "ln_final.hook_scale            (1, 16, 1)\n",
      "ln_final.hook_normalized       (1, 16, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377977-fe3b-45dd-9d00-1c19e5366038",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Test functioning of prop_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = [(0, 0, 0), (1, 1, 1)]\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "\n",
    "text = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "encoding = get_encoding(text, model.tokenizer, device)\n",
    "encoding_idxs, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "input_shape = encoding_idxs.size()\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "out_decomps, target_decomps, _ = prop_GPT(encoding_idxs, extended_attention_mask, model, source_list, target_nodes, device=device, patched_values=None, mean_ablated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d09d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(512, 50257)\n",
      "[[ 5.5012274  8.864304   3.4959507 ... -4.0528517 -2.6054635  7.205061 ]\n",
      " [ 5.5009346  8.865846   3.4954956 ... -4.053485  -2.6107361  7.206545 ]\n",
      " [ 5.499007   8.864225   3.4940565 ... -4.0518312 -2.6099834  7.2044487]\n",
      " ...\n",
      " [ 5.4935718  8.862474   3.4887767 ... -4.0536976 -2.6203256  7.198222 ]\n",
      " [ 5.4935718  8.862474   3.4887767 ... -4.0536976 -2.6203256  7.198222 ]\n",
      " [ 5.4935718  8.862474   3.4887767 ... -4.0536976 -2.6203256  7.198222 ]]\n"
     ]
    }
   ],
   "source": [
    "print(len(out_decomps)) # one per source node\n",
    "print(out_decomps[0].rel) \n",
    "print(out_decomps[0].rel.shape) # 512, d_vocab-- the effective output is this[-1], but most of this is zeroes, and is only nonzero at target node indices\n",
    "print(out_decomps[0].irrel)\n",
    "# TODO: given the formula we have calculated for relevance, is there a more compact representation of the output available?\n",
    "\n",
    "# TODO: check correctness of various basic operations: if no target node specified, should irrel be the same as output? should sum over all input source nodes of output decomposition be equal to original output?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b9226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "061cbe24",
   "metadata": {},
   "source": [
    "## Test correctness of forward pass\n",
    "\n",
    "Given that the above code runs, the forward pass \"type checks\", e.g, it's at least able to run. We still have to test correctness; this is especially hard given that TL GPT has so many discrepancies from HF BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb32d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_same(a, b, atol=1e-4, rtol=1e-3):\n",
    "    if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\n",
    "        comparison = torch.isclose(a, b, atol, rtol)\n",
    "        print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "        return\n",
    "    comparison = np.isclose(a, b, atol, rtol)\n",
    "    print(f\"{comparison.sum()/comparison.size:.2%} of the values are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855602d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "# This loosely verifies that the out-decomposition's rel + irrel components form the output of the forward pass\n",
    "compare_same(out_decomps[0].rel + out_decomps[0].irrel, out_decomps[1].rel + out_decomps[1].irrel)\n",
    "\n",
    "# compare_same(cache['blocks.0.attn.hook_q'], cache['blocks.0.attn.hook_k']) # checking that there aren't the inputs, because the source code names them as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a376f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of the values are correct\n",
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "# compare_same(cache['hook_embed'], cache['blocks.0.hook_resid_pre'])\n",
    "# This just verifies an assumption about the input conventions to the model.\n",
    "in_0 = cache['blocks.0.hook_resid_pre']\n",
    "compare_same(cache['hook_embed'] + cache['hook_pos_embed'], cache['blocks.0.hook_resid_pre'])\n",
    "\n",
    "# Ensure that the encodings are the same (they won't be if you set prepend_bos=True for one and not the other!)\n",
    "# print(tokens)\n",
    "# print(encoding.input_ids)\n",
    "\n",
    "# Note that model.embed alone isn't the same thing as what we might normally call \"the embedding\"!\n",
    "embedding_output = model.embed(encoding.input_ids) + model.pos_embed(encoding.input_ids)\n",
    "compare_same(in_0, embedding_output[:, :16, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f2c3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 768, 64])\n",
      "torch.Size([12, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "# Extra analysis of value matrix calcuation\n",
    "print(model.blocks[0].attn.W_V.shape)\n",
    "print(model.blocks[0].attn.W_O.shape)\n",
    "# print(model.blocks[0].attn.hook_pattern) # 1, 12, seq_len, seq_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0edc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Z' matrix:\n",
      "99.71% of the values are correct\n",
      "Output matrix:\n",
      "99.89% of the values are correct\n",
      "99.89% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "# Test equivalence of attention layer\n",
    "from fancy_einsum import einsum\n",
    "'''\n",
    "blocks.0.attn.hook_q           (1, 17, 12, 64)\n",
    "blocks.0.attn.hook_k           (1, 17, 12, 64)\n",
    "blocks.0.attn.hook_v           (1, 17, 12, 64)\n",
    "blocks.0.attn.hook_attn_scores (1, 12, 17, 17)\n",
    "blocks.0.attn.hook_pattern     (1, 12, 17, 17)\n",
    "blocks.0.attn.hook_z           (1, 17, 12, 64)\n",
    "'''\n",
    "\n",
    "input_shape = encoding['input_ids'].size()\n",
    "\n",
    "attention_mask = get_extended_attention_mask(encoding['attention_mask'], \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "sh = list(embedding_output.shape)\n",
    "rel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel[:] = embedding_output[:]\n",
    "layer_module = model.blocks[0]\n",
    "attention_module = layer_module.attn\n",
    "\n",
    "\n",
    "rel_ln, irrel_ln = prop_layer_norm(rel, irrel, GPTLayerNormWrapper(layer_module.ln1))\n",
    "'''\n",
    "print(\"LayerNorm: \")\n",
    "compare_same(cache['blocks.0.ln1.hook_normalized'], (rel_ln + irrel_ln)[0, :16, :])\n",
    "\n",
    "desired_value_output = cache['blocks.0.attn.hook_v']\n",
    "rel_value, irrel_value = prop_linear(rel_ln, irrel_ln, GPTAttentionWrapper(attention_module, layer_module.ln2).value)\n",
    "value = rel_value + irrel_value\n",
    "print(\"Value matrix:\")\n",
    "# desired_value_output = desired_value_output.transpose(-1, -2)\n",
    "old_shape = desired_value_output.size()\n",
    "new_shape = old_shape[:-2] + (old_shape[-2] * old_shape[-1],)\n",
    "desired_value_output = desired_value_output.reshape(new_shape)\n",
    "print(desired_value_output.shape)\n",
    "print(value.shape)\n",
    "\n",
    "compare_same(value[0, :16, :], desired_value_output, rtol = 1e-2) # some elements differ by more than 1e-3 in proportion, not great\n",
    "'''\n",
    "\n",
    "# desired_block_output = cache['blocks.0.hook_resid_mid']\n",
    "\n",
    "desired_block_output = cache['blocks.0.hook_attn_out']\n",
    "desired_attention_pattern = cache['blocks.0.attn.hook_pattern']\n",
    "# level = 0\n",
    "# source_node_list = [(0, 0, 0), (1, 1, 1)]\n",
    "# target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "attn_wrapper = GPTAttentionWrapper(attention_module)\n",
    "rel_summed_values, irrel_summed_values, returned_att_probs = prop_self_attention_hh(rel_ln, irrel_ln, attention_mask, \n",
    "                                                                        None,\n",
    "                                                                        attn_wrapper,\n",
    "                                                                        att_probs=None, output_att_prob=True)\n",
    "print(\"'Z' matrix:\")\n",
    "desired_z = cache['blocks.0.attn.hook_z']\n",
    "old_shape = desired_z.size()\n",
    "new_shape = old_shape[:-2] + (old_shape[-2] * old_shape[-1],)\n",
    "desired_z = desired_z.reshape(new_shape)\n",
    "z = rel_summed_values + irrel_summed_values\n",
    "compare_same(desired_z, z[:, :16, :])\n",
    "\n",
    "rel_attn_residual, irrel_attn_residual = prop_linear(rel_summed_values, irrel_summed_values, attn_wrapper.output)\n",
    "\n",
    "print(\"Output matrix:\")\n",
    "output = rel_attn_residual + irrel_attn_residual\n",
    "# output = rel_attn_residual + irrel_attn_residual + rel_ln + irrel_ln\n",
    "compare_same(output[:, :16, :], desired_block_output)\n",
    "\n",
    "\n",
    "desired_mid_output = cache['blocks.0.hook_resid_mid']\n",
    "rel_mid, irrel_mid = rel + rel_attn_residual, irrel + irrel_attn_residual\n",
    "mid_output = rel_mid + irrel_mid\n",
    "compare_same(mid_output[:, :16, :], desired_mid_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ab56041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.97% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "# Test equivalence of entire transformer block\n",
    "in_1 = cache['blocks.1.hook_resid_pre']\n",
    "input_shape = encoding['input_ids'].size()\n",
    "\n",
    "extended_attention_mask = get_extended_attention_mask(encoding['attention_mask'], \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "sh = list(embedding_output.shape)\n",
    "rel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel = torch.zeros(sh, dtype = embedding_output.dtype, device = device)\n",
    "irrel[:] = embedding_output[:]\n",
    "layer_module = model.blocks[0]\n",
    "\n",
    "source_node_list = [(0, 0, 0), (1, 1, 1)]\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "rel_out, irrel_out, layer_target_decomps, _ = prop_GPT_layer_hh(rel, irrel, extended_attention_mask, \n",
    "                                                                                 None, source_node_list, \n",
    "                                                                                 target_nodes, 0, \n",
    "                                                                                 None,\n",
    "                                                                                 layer_module, \n",
    "                                                                                 device,\n",
    "                                                                                 None, False,\n",
    "                                                                                 mean_ablated=False)\n",
    "\n",
    "layer0_output = rel_out + irrel_out\n",
    "compare_same(in_1, layer0_output[:, :16, :])\n",
    "\n",
    "# want to check: layer_target_decomps is what we think it is (currently gated by, i didn't implement the decomposition part)\n",
    "# also, rel + irrel sums to the correct output\n",
    "# layer_target_decomps[0].rel + layer_target_decomps[0].irrel == layer_target_decomps[1].rel + layer_target_decomps[1].irrel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c0bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 50257])\n",
      "(512, 50257)\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test equivalence of model run to logits\n",
    "print(logits.shape)\n",
    "model_out = out_decomps[0].rel + out_decomps[0].irrel\n",
    "print(model_out.shape)\n",
    "compare_same(logits[0], model_out[:16, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43bc3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
