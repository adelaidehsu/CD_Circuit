{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a120102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# CD-T Imports\n",
    "import math\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from argparse import Namespace\n",
    "from methods.bag_of_ngrams.processing import cleanReports, cleanSplit, stripChars\n",
    "from pyfunctions.general import extractListFromDic, readJson, combine_token_attn, compute_word_intervals\n",
    "from pyfunctions.pathology import extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n",
    "from pyfunctions.patch_hh import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d6ada4-4781-4789-b3b1-1d044c11b3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ff76d8e4390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Model Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3183be1-3bf6-4f5a-8134-9bdd83db0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4838142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "820d21e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5069ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd \"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b375528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m320.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (0.24.1)\n",
      "Requirement already satisfied: packaging in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/georgiazhou/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.4/392.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.4 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08241bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 3.46k/3.46k [00:00<00:00, 10.5MB/s]\n",
      "Downloading data: 100%|██████████| 5.84M/5.84M [00:01<00:00, 4.59MB/s]\n",
      "Downloading data: 100%|██████████| 756M/756M [01:27<00:00, 8.65MB/s] \n",
      "Generating train split: 100%|██████████| 26210000/26210000 [00:03<00:00, 7827513.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ioi_dataset = load_dataset(\"fahamu/ioi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8053ccd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args['model_type'] == 'bert':\n",
    "    bert_path = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "elif args['model_type'] == 'medical_bert':\n",
    "    bert_path = f\"{base_dir}/models/pretrained/bert_pretrain_output_all_notes_150000/\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_path, local_files_only=True)\n",
    "elif args['model_type'] == 'pubmed_bert':\n",
    "    bert_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "elif args['model_type'] == 'pubmed_bert_full':\n",
    "    bert_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "elif args['model_type'] == 'biobert':\n",
    "    bert_path = \"dmis-lab/biobert-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "elif args['model_type'] == 'clinical_biobert':\n",
    "    bert_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377977-fe3b-45dd-9d00-1c19e5366038",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba4d6b-b67c-4ae9-8bc2-b4acb7ce1a65",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "you can cutomize the code here to read in your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b295089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 0, 0, 13, 14, 9, 11, 14, 1, 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = IOIDataset(N=500, prompt_type=\"ABBA\", tokenizer=tokenizer)\n",
    "#data.tokenized_prompts\n",
    "data.ioi_prompts[0]\n",
    "[x['TEMPLATE_IDX'] for x in data.ioi_prompts[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea84464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1345 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2066 517 324\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "#field = 'PrimaryGleason' # out of PrimaryGleason, SecondaryGleason', 'MarginStatusNone', 'SeminalVesicleNone'\n",
    "path = f\"../data/prostate.json\"\n",
    "data = readJson(path)\n",
    "\n",
    "# Clean reports\n",
    "data = cleanSplit(data, stripChars)\n",
    "data['dev_test'] = cleanReports(data['dev_test'], stripChars)\n",
    "data = fixLabel(data)\n",
    "\n",
    "train_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['train']]\n",
    "val_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['val']]\n",
    "test_documents = [extract_synoptic(patient['document'].lower(), tokenizer) for patient in data['test']]\n",
    "print(len(train_documents), len(val_documents),len(test_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae7a95c-3b73-42b0-84fe-9aa08fd927e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_labels = [patient['labels'][args['field']] for patient in data['train']]\n",
    "val_labels = [patient['labels'][args['field']] for patient in data['val']]\n",
    "test_labels = [patient['labels'][args['field']] for patient in data['test']]\n",
    "\n",
    "train_documents, train_labels = exclude_labels(train_documents, train_labels)\n",
    "val_documents, val_labels = exclude_labels(val_documents, val_labels)\n",
    "test_documents, test_labels = exclude_labels(test_documents, test_labels)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "\n",
    "# Map raw label to processed label\n",
    "le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "le_dict = {str(key):le_dict[key] for key in le_dict}\n",
    "\n",
    "for label in val_labels + test_labels:\n",
    "    if str(label) not in le_dict:\n",
    "        le_dict[str(label)] = len(le_dict)\n",
    "\n",
    "# Map processed label back to raw label\n",
    "inv_le_dict = {v: k for k, v in le_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23e85ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents_full = train_documents + val_documents + test_documents\n",
    "labels_full = train_labels + val_labels + test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd45f9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load finetuned model\n",
    "model_path = f\"{base_dir}/models/{args['task']}/{args['model_type']}_{args['field']}\"\n",
    "checkpoint_file = f\"{model_path}/save_output\"\n",
    "config_file = f\"{model_path}/save_output/config.json\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint_file, num_labels=len(le_dict), output_hidden_states=True)\n",
    "\n",
    "model = model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76277da4-2c38-4e47-8c4a-1c5b462943b7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Head to head direct influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b008fcb-8bcf-4033-b58a-bcb5aadc5462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the pre-calculated mean head response\n",
    "path = f\"{base_dir}/output/{args['task']}/{args['model_type']}_{args['field']}/h_to_logits\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(path, f\"mean_head_out_res_500.pkl\"), 'rb') as handle:\n",
    "    back = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693cd0e8-2fd3-4e60-bb2f-00efa1356dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_hh_at_pos(encoding, model, target_nodes, pos=0, mean_act=None, mean_ablated=False):\n",
    "    pos_specific_hs = [\n",
    "        [i for i in range(12)],\n",
    "        [pos],\n",
    "        [i for i in range(12)]\n",
    "    ]\n",
    "    all_heads = list(itertools.product(*pos_specific_hs))\n",
    "\n",
    "    # patch one node at a time\n",
    "    h_ctbn_list = []\n",
    "    \n",
    "    source_list = [[node] for node in all_heads if node not in target_nodes]\n",
    "    out_decomps, target_decomps = prop_classifier_model_hh_batched(encoding, model, source_list, target_nodes,\n",
    "                                                                   device=device,\n",
    "                                                                   patched_values=mean_act, mean_ablated=True)\n",
    "    for i, _ in enumerate(source_list):\n",
    "        ctbn = 0\n",
    "        for l in range(12):\n",
    "            if target_decomps[l][i][0].shape[0] != 0:\n",
    "                rel_part = np.mean(abs(target_decomps[l][i][0]))\n",
    "                irrel_part = np.mean(abs(target_decomps[l][i][1]))\n",
    "                ctbn += rel_part / abs(rel_part + irrel_part) * 100\n",
    "        h_ctbn_list.append(ctbn)\n",
    "        \n",
    "    return source_list, h_ctbn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff0fe2e-2ba8-4470-9dcd-7c65b048ccfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform on one doc as an example\n",
    "text = documents_full[0]\n",
    "label = labels_full[0]\n",
    "encoding = get_encoding(text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd8d1919-0e91-4f59-a07d-05847645d6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [1:52:20<00:00, 13.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# perform one iteration measuring effect of the source nodes to target nodes as an example\n",
    "# note that target nodes get updated in each iteration\n",
    "\n",
    "target_nodes = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "\n",
    "all_source_hs = []\n",
    "all_htbn = []\n",
    "for pos in tqdm.tqdm(range(512)):\n",
    "    with torch.no_grad():\n",
    "        source_list, h_ctbn_list = patch_hh_at_pos(encoding, model, target_nodes, pos=pos, mean_act=back, mean_ablated=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    all_source_hs.append(source_list)\n",
    "    all_htbn.append(h_ctbn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e6c0bf3b-0617-4884-80d7-8091ddedce64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_ctbn = [c for sublist in all_htbn for c in sublist]\n",
    "flat_source_h = [c for sublist in all_source_hs for c in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d0e92299-54f4-4428-bf98-b920b237831e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_idx = sorted(range(len(flat_ctbn)), key=lambda i: flat_ctbn[i])[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d48ead3-05fc-43d4-b90f-a32d822bc2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 82, 4)] 36.659424751996994\n",
      "[(5, 82, 4)] 42.90880411863327\n",
      "[(3, 82, 0)] 51.443591713905334\n",
      "[(4, 82, 0)] 87.46089041233063\n",
      "[(5, 82, 0)] 103.08798849582672\n",
      "[(6, 82, 0)] 132.23715126514435\n"
     ]
    }
   ],
   "source": [
    "for i in top_idx:\n",
    "    print(flat_source_h[i], flat_ctbn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "98bc384b-e405-4815-b919-3b6d03739e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the identified heads\n",
    "path = f\"{base_dir}/output/{args['task']}/{args['model_type']}_{args['field']}/h3\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(path, f\"flat_source_h.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(flat_source_h, handle)\n",
    "    \n",
    "with open(os.path.join(path, f\"flat_source_h.pkl\"), 'rb') as handle:\n",
    "    back = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515c210-6040-41c2-a3a5-9d55d7112863",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Examine the attended words by the identified heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1bf9c5-979b-4473-8958-e34adab200a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_attended_tokens_hh(positives_heads, device, tokenizer, N=100, Z_thres=2, percentile=75, use_perc=False):\n",
    "    index_lst = random.sample(range(0, len(documents_full)), N)\n",
    "    docs = [documents_full[i] for i in index_lst]\n",
    "    \n",
    "    collect = collections.defaultdict(int)\n",
    "    for doc in docs:\n",
    "        encoding = get_encoding(doc, tokenizer, device)\n",
    "        \n",
    "        _, _, raw_att_probs_lst = prop_classifier_model_hh(encoding, model, [[]], [], device=device, output_att_prob=True)\n",
    "        raw_att_probs = torch.stack(raw_att_probs_lst).cpu().numpy()\n",
    "\n",
    "        avg_att_m = np.zeros((512))\n",
    "        for level, pos, h in positives_heads:\n",
    "            att_m = raw_att_probs[level, h, pos, :]\n",
    "            avg_att_m += att_m\n",
    "\n",
    "        avg_att_m /= len(positives)\n",
    "        \n",
    "        # convert to word level\n",
    "        interval_dict, word_lst = compute_word_intervals(encoding, tokenizer)\n",
    "        word_att_m = combine_token_attn(interval_dict, avg_att_m)\n",
    "        \n",
    "        if use_perc:\n",
    "            perc_cutoff = np.percentile(word_att_m, percentile)\n",
    "            positive_words = np.where(word_att_m > perc_cutoff)\n",
    "        else:\n",
    "            Z = (word_att_m - np.mean(word_att_m)) / np.std(word_att_m)\n",
    "            positive_words = np.where(Z > Z_thres)\n",
    "        \n",
    "        for w_idx in positive_words[0]:\n",
    "            w = word_lst[w_idx]\n",
    "            #collect[w] += 1\n",
    "            collect[w] += word_att_m[w_idx]\n",
    "            \n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b2bbf39-9363-45ae-b516-2da5e1d9d773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_attended_tokens_hh_rm_pos(positives_heads, device, tokenizer, N=100, Z_thres=2, percentile=75, use_perc=False):\n",
    "    index_lst = random.sample(range(0, len(documents_full)), N)\n",
    "    docs = [documents_full[i] for i in index_lst]\n",
    "    \n",
    "    collect = collections.defaultdict(int)\n",
    "    for doc in docs:\n",
    "        encoding = get_encoding(doc, tokenizer, device)\n",
    "        \n",
    "        _, _, raw_att_probs_lst = prop_classifier_model_hh(encoding, model, [[]], [], device=device, output_att_prob=True)\n",
    "        raw_att_probs = torch.stack(raw_att_probs_lst).cpu().numpy()\n",
    "\n",
    "        avg_att_m = np.zeros((512))\n",
    "        for level, _, h in positives_heads:\n",
    "            att_m = raw_att_probs[level, h, :, :]\n",
    "            #att_m = np.mean(att_m, axis=0)\n",
    "            max_row = np.unravel_index(np.argmax(att_m, axis=None), att_m.shape)[0]\n",
    "            avg_att_m += att_m[max_row, :]\n",
    "\n",
    "        avg_att_m /= len(positives)\n",
    "        \n",
    "        # convert to word level\n",
    "        interval_dict, word_lst = compute_word_intervals(encoding, tokenizer)\n",
    "        word_att_m = combine_token_attn(interval_dict, avg_att_m)\n",
    "        \n",
    "        if use_perc:\n",
    "            perc_cutoff = np.percentile(word_att_m, percentile)\n",
    "            positive_words = np.where(word_att_m > perc_cutoff)\n",
    "        else:\n",
    "            Z = (word_att_m - np.mean(word_att_m)) / np.std(word_att_m)\n",
    "            positive_words = np.where(Z > Z_thres)\n",
    "        \n",
    "        for w_idx in positive_words[0]:\n",
    "            w = word_lst[w_idx]\n",
    "            #collect[w] += 1\n",
    "            collect[w] += word_att_m[w_idx]\n",
    "            \n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfd9dacb-2fb8-40e3-bdd1-94a1f62b1973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negatives = [(11, 2), (11, 5), (11, 6), (11, 9), (11, 10), (11, 11),\n",
    "             (10, 3), (10, 4), (10, 5), (10, 6), (10, 9), (10, 10), (10, 11),\n",
    "             (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 8), (9, 9), (9, 10), (9, 11),\n",
    "             (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11),\n",
    "             (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 7), (7, 8), (7, 9), (7, 10),\n",
    "             (6, 1), (6, 2), (6, 3), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11),\n",
    "             (5, 1), (5, 2), (5, 3), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11),\n",
    "             (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11),\n",
    "             (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11),\n",
    "             (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11),\n",
    "             (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 0), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11),\n",
    "             (0, 0), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 8), (0, 10), (0, 11),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36cebd9e-5d02-41cc-923b-65e88bf24b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the identified attn heads using CD-T\n",
    "pos_specific_hs = [\n",
    "            [i for i in range(12)],\n",
    "            [i for i in range(512)],\n",
    "            [i for i in range(12)]\n",
    "        ]\n",
    "all_heads = list(itertools.product(*pos_specific_hs))\n",
    "random_heads = random.sample(all_heads, 6)\n",
    "positives = random_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87528ebc-415b-45f6-a6bc-5cdf193d482b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positives = [(1, 169, 2), (2, 169, 2), (2, 169, 3), (4, 169, 8), (1, 411, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc7df4ef-9292-4b83-8201-2ad57cfee8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_attended_token_freq = collect_attended_tokens_hh_rm_pos(positives, device, tokenizer, N=200, use_perc=True)\n",
    "positive_attended_token_freq = sorted(positive_attended_token_freq.items(), key=lambda k_v: k_v[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42c0c0cc-0f63-466d-b586-822bbca6e8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#h1 = [(10, 82, 0), (10, 61, 8), (10, 82, 7), (10, 176, 2), (10, 467, 1), (10, 91, 7)]\n",
    "#h2 = [(7, 82, 11), (7, 82, 0), (7, 82, 6), (9, 82, 0), (9, 91, 7), (8, 82, 0)]\n",
    "#h3 = [(6, 82, 4), (5, 82, 4), (3, 82, 0), (4, 82, 0), (5, 82, 0), (6, 82, 0)]\n",
    "#positives = [(0, 82, 9), (0, 82, 1), (0, 82, 7), (1, 82, 6), (0, 82, 6), (2, 82, 0)]\n",
    "positive_attended_token_freq = collect_attended_tokens_hh(positives, device, tokenizer, N=500, use_perc=True)\n",
    "positive_attended_token_freq = sorted(positive_attended_token_freq.items(), key=lambda k_v: k_v[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5fb143fa-8d8d-4185-8931-53d94c329144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pp15_h2.json', 'w') as fp:\n",
    "    json.dump(positive_attended_token_freq, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb563fe-5586-4600-a700-a5fe4c23d87d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4494d04-0b59-4e12-ae4f-660c1680398e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = documents_full[0]\n",
    "label = labels_full[0]\n",
    "encoding = get_encoding(text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ccb2c31-7d29-4307-8309-76454da052ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "source_list_30 = [#list(itertools.product(range(12), range(512), range(12))), \n",
    "                  # list(itertools.product(range(12), range(70, 85), range(12))), \n",
    "                  # [(11, 0, i) for i in range(12)]\n",
    "                  [(0, 0, 0)]] * 30\n",
    "source_list_60 = [#list(itertools.product(range(12), range(512), range(12))), \n",
    "                  # list(itertools.product(range(12), range(70, 85), range(12))), \n",
    "                  # [(11, 0, i) for i in range(12)]\n",
    "                  [(0, 0, 0)], []] * 30\n",
    "\"\"\"\n",
    "target_nodes = [(11, 8), (11, 0), (11, 1), (11, 4), (11, 3), (11, 7)]\n",
    "source_list = [[(5, 7, 0)], [(5, 5, 0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140cd70-be4d-43ac-a2dc-fd62f8e9d73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_list = [[(5, 7, 0)], [(5, 5, 0)]]\n",
    "out_decomps, target_decomps = prop_classifier_model_hh_batched(encoding, model, source_list, target_nodes, patched_values=back, mean_ablated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e27d0-769c-4c06-9114-e567034a1372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_decomps, target_decomps, _ = prop_classifier_model_hh(encoding, model, source_list, target_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cdcc84d2-f700-4b3b-a62d-a397b657dbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([-0.0171,  0.0302, -0.0149], device='cuda:0'),\n",
       "  tensor([-3.2685,  6.1339, -3.2953], device='cuda:0')),\n",
       " (tensor([-0.0179,  0.0306, -0.0147], device='cuda:0'),\n",
       "  tensor([-3.2677,  6.1335, -3.2955], device='cuda:0'))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_decomps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ccea5db-d9ad-4468-8e36-3f0f0903dd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_decomps[11][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "882e2b06-770b-4e60-8031-76693614b5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0, 64])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_decomps[0][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41caeed-2bc6-4d62-9ff7-8b382ebfe062",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Function description:\n",
    "\n",
    "prop_classifier_model_hh_batched(encoding, model, source_list, target_nodes):\n",
    "\n",
    "- encoding - Encoding given by tokenizer\n",
    "- model - BERT model\n",
    "- source_list - List of lists where each list consists of tuples (layer, position, head) indexing a particular attention head whose influence is to be calculated\n",
    "- target_nodes - A single list of tuples (layer, position, head) containing attention heads on whom the influence is to be measured\n",
    "- num_at_time (optional) - Number of source_lists to be processed in a batch\n",
    "- n_layers - Number of layers\n",
    "- att_list - Attention probabilities if precomputed\n",
    "\n",
    "Output consists of two lists - out_decomps and target_decomps:\n",
    "- out_decomps - Consists of a list of tuples (rel, irrel) reflecting the decomposition of the _output_\n",
    "- target_decomps - A list containining 12 (one for each layer) where each list is of length len(source_list). For any layer l, each entry of target_decomps[l] is a tuple (rel, irrel) decomposition of the target nodes at that layer for the corresponding set of source nodes. rel, irrel are of dimension #number of target nodes in layer l x head_size and the ordering of the target nodes in this layer is the same as provided "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
